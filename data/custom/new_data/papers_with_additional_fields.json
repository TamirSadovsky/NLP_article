{
    "W11-1305": {
        "title": "Shared task system description: Frustratingly hard compositionality prediction authors: Anders Johannsen, Hector Martinez, Christian Rishoj, Anders Sogaard citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 1 isOpenAccess: False isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 9 numCiting: 4 s2FieldsOfStudy: Computer Science, Psychology topics: Experiment, Cross-validation (statistics), Support vector machine, WordNet, Hyphenation algorithm, Baseline (configuration management), Cross-Sectional Studies, Programming Languages, Equivalent Weight venue:  year: 2011",
        "abstract": "We considered a wide range of features for the DiSCo 2011 shared task about compositionality prediction for word pairs, including COALS-based endocentricity scores, compositionality scores based on distributional clusters, statistics about wordnet-induced paraphrases, hyphenation, and the likelihood of long translation equivalents in other languages. Many of the features we considered correlated significantly with human compositionality scores, but in support vector regression experiments we obtained the best results using only COALS-based endocentricity scores. Our system was nevertheless the best performing system in the shared task, and average error reductions over a simple baseline in cross-validation were 13.7% for English and 50.1% for German."
    },
    "W95-0110": {
        "title": "Inverse Document Frequency (IDF): A Measure of Deviations from Poisson authors: Kenneth Ward Church, W. Gale citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 31 isOpenAccess: False isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 247 numCiting: 5 s2FieldsOfStudy: Computer Science, Economics topics: Tf-idf, Text corpus, Information retrieval, Randomness venue: VLC@ACL year: 1995",
        "abstract": "Low frequency words tend to be rich in content, and vice versa. But not all equally frequent words are equally meaningful. We will use inverse document frequency (IDF), a quantity borrowed from Information Retrieval, to distinguish words like somewhat and boycott. Both somewhat and boycott appeared approximately 1000 times in a corpus of 1989 Associated Press articles, but boycott is a better keyword because its IDF is farther from what would be expected by chance (Poisson). Word frequency is commonly used in all sorts of natural language applications. The practice implicitly assumes that words (and ngrams) are distributed by a single parameter distribution such as a Poisson or a Binomial. But we find that these distributions do not fit the data very well. Both the Poisson and Binomial assume that the variance over documents is no larger than the mean, and yet, we find that it can be quite a bit larger, especially for interesting words such as boycott where there are hidden variables such as topic that conspire to undermine the independence assumption behind the Poisson and the Binomial. Much better fits are obtained by introducing a second parameter such as inverse document frequency (IDF). Inverse document frequency (IDF) is commonly used in Information Retrieval (Sparck Jones, 1972). IDF is defined as \u2014log2df/D, where D is the number of documents in the collection and df w is the document frequency, the number of documents that contain w. Obviously, there is a strong relationship between document frequency, df, and word frequency, f. The relationship is shown in Figure 1, a plot of log of and IDF for 193 words selected from a 50 million word corpus of 1989 Associated Press (AP) Newswire stories (D = 85,432 stories). Although logiof,\u201e is highly correlated with IDF (p = \u2014 0.994), it would be a mistake to assume that the two variables are completely predictable from one another. Indeed, the experience of the Information Retrieval community has indicated that IDF is a very useful quantity. Attempts to replace IDF with fw (or some simple transform off) have not been very successful. Figure 2 shows one such attempt. It compares the observed IDF with IDF, an estimate based on f Assume that a document is merely a &quot;bag of words&quot; with no interesting structure (content). Words are randomly generated by a Poisson process, it. The probability of k instances of a word w is ic(9,k) fw where 0= In particular, the probability that w will not be found in a document is rc(0,0). Conversely, the probability of at least one w is 1 \u2014 TC(0, 0). And therefore, IDF ought to be: The prediction errors are shown in more detail in Figure 3, which plots the residual IDF (difference between predicted and observed) as a function of log iof,\u201e for the same 193 words shown in Figure 2. The prediction errors are relatively large in the middle of the frequency range, and smaller at both ends. Unfortunately, we believe the words in the middle are often the most important words for Information Retrieval purposes."
    },
    "W09-0510": {
        "title": "Incrementality, Speaker-Hearer Switching and the Disambiguation Challenge authors: Ruth Kempson, E. Gregoromichelaki, Yo Sato citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 0 isOpenAccess: True isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 11 numCiting: 23 s2FieldsOfStudy: Computer Science, Psychology topics: Word-sense disambiguation, Adaptive grammar, Parsing venue:  year: 2009",
        "abstract": "Taking so-called split utterances as our point of departure, we argue that a new perspective on the major challenge of disambiguation becomes available, given a framework in which both parsing and generation incrementally involve the same mechanisms for constructing trees reflecting interpretation (Dynamic Syntax: (Cann et al., 2005; Kempson et al., 2001)). With all dependencies, syntactic, semantic and pragmatic, defined in terms of incremental progressive tree growth, the phenomenon of speaker/hearer role-switch emerges as an immediate consequence, with the potential for clarification, acknowledgement, correction, all available incrementally at any sub-sentential point in the interpretation process. Accordingly, at all intermediate points where interpretation of an utterance subpart is not fully determined for the hearer in context, uncertainty can be resolved immediately by suitable clarification/correction/repair/extension as an exchange between interlocutors. The result is a major check on the combinatorial explosion of alternative structures and interpretations at each choice point, and the basis for a model of how interpretation in context can be established without either party having to make assumptions about what information they and their interlocutor share in resolving ambiguities."
    },
    "W03-1006": {
        "title": "Use of Deep Linguistic Features for the Recognition and Labeling of Semantic Arguments authors: John Chen, Owen Rambow citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 7 isOpenAccess: True isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 101 numCiting: 15 s2FieldsOfStudy: Computer Science, Computer Science, Linguistics topics: PropBank, Text corpus, Semantic role labeling, Parsing venue: Conference on Empirical Methods in Natural Language Processing year: 2003",
        "abstract": "We use deep linguistic features to predict semantic roles on syntactic arguments, and show that these perform considerably better than surface-oriented features. We also show that predicting labels from a \u201clightweight\u201d parser that generates deep syntactic features performs comparably to using a full parser that generates only surface syntactic features."
    },
    "W03-1007": {
        "title": "Maximum Entropy Models for FrameNet Classification authors: Michael Fleischman, Namhee Kwon, E. Hovy citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 1 isOpenAccess: True isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 89 numCiting: 8 s2FieldsOfStudy: Computer Science, Computer Science topics: FrameNet, Statistical classification, Windows ME, HTML element, Maximum entropy spectral estimation, Test set, Linear interpolation, Test data, Principle of maximum entropy, Experiment venue: Conference on Empirical Methods in Natural Language Processing year: 2003",
        "abstract": "The development of FrameNet, a large database of semantically annotated sentences, has primed research into statistical methods for semantic tagging. We advance previous work by adopting a Maximum Entropy approach and by using previous tag information to find the highest probability tag sequence for a given sentence. Further we examine the use of sentence level syntactic pattern features to increase performance. We analyze our strategy on both human annotated and automatically identified frame elements, and compare performance to previous work on identical test data. Experiments indicate a statistically significant improvement (p<0.01) of over 6%."
    },
    "S14-2078": {
        "title": "NTNU: Measuring Semantic Similarity with Sublexical Feature Representations and Soft Cardinality authors: Andre Lynum, Partha Pakray, Bjorn Gamback, Sergio Jimenez citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 3 isOpenAccess: True isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 18 numCiting: 21 s2FieldsOfStudy: Computer Science, Computer Science topics: Semantic similarity, SemEval, Support vector machine, N-gram, Baseline (configuration management) venue: International Workshop on Semantic Evaluation year: 2014",
        "abstract": "The paper describes the approaches taken by the NTNU team to the SemEval 2014 Semantic Textual Similarity shared task. The solutions combine measures based on lexical soft cardinality and character n-gram feature representations with lexical distance metrics from TakeLab\u2019s baseline system. The final NTNU system is based on bagged support vector machine regression over the datasets from previous shared tasks and shows highly competitive performance, being the best system on three of the datasets and third best overall (on weighted mean over all six datasets)."
    },
    "S14-2079": {
        "title": "OPI: Semeval-2014 Task 3 System Description authors: Marek Kozlowski citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 1 isOpenAccess: True isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 3 numCiting: 17 s2FieldsOfStudy: Computer Science, Computer Science topics: Semantic similarity, Wikipedia, BabelNet, WordNet, Thesaurus, Text corpus, Dictionary, Unsupervised learning, Parsing, Ontology (information science), Bitext word alignment, Test set, Categorization, Embedded system, Human-readable medium, SemEval, Microsoft Word for Mac, Raw image format, Knowledge management, Algorithm venue: International Workshop on Semantic Evaluation year: 2014",
        "abstract": "In this paper, we describe the OPI system participating in the Semeval-2014 task 3 Cross-Level Semantic Similarity. Our approach is knowledge-poor, there is no exploitation of any structured knowledge resources as Wikipedia, WordNet or BabelNet. The method is also fully unsupervised, the training set is only used in order to tune the system. System measures the semantic similarity of texts using corpusbased measures of termsets similarity."
    },
    "W03-1002": {
        "title": "Statistical Machine Translation Using Coercive Two-Level Syntactic Transduction authors: C. Schafer, David Yarowsky citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 0 isOpenAccess: True isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 20 numCiting: 22 s2FieldsOfStudy: Computer Science, Computer Science topics: Statistical machine translation, Transduction (machine learning), Phrase chunking, BLEU, Part-of-speech tagging, Shallow parsing venue: Conference on Empirical Methods in Natural Language Processing year: 2003",
        "abstract": "We define, implement and evaluate a novel model for statistical machine translation, which is based on shallow syntactic analysis (part-of-speech tagging and phrase chunking) in both the source and target languages. It is able to model long-distance constituent motion and other syntactic phenomena without requiring a full parse in either language. We also examine aspects of lexical transfer, suggesting and exploring a concept of translation coercion across parts of speech, as well as a transfer model based on lemma-to-lemma translation probabilities, which holds promise for improving machine translation of low-density languages. Experiments are performed in both Arabic-to-English and French-to-English translation demonstrating the efficacy of the proposed techniques. Performance is automatically evaluated via the Bleu score metric."
    },
    "W03-1003": {
        "title": "Cross-Lingual Lexical Triggers in Statistical Language Modeling authors: Woosung Kim, S. Khudanpur citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 2 isOpenAccess: True isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 19 numCiting: 26 s2FieldsOfStudy: Computer Science, Computer Science topics: Precipitating Factors, Super Robot Monkey Team Hyperforce Go!, Machine translation, Language model, Programming Languages, Text corpus, Perplexity, Statistical model, Natural language processing, Body of uterus, Lexicon, Cross-language information retrieval, Phrase chunking, Bottleneck (engineering), Automatic speech recognition, Tongue, Bilingual dictionary, Parallel text, Word lists by frequency, Experiment, Vocabulary, Angular defect, Neural coding, Acoustic cryptanalysis, Online and offline, Simulation, Inclusion Body Myositis (disorder), Surround sound, N-gram, Estimated, Acclimatization, License, Automated system recovery, Priming Exercise, Alignment, Polyglot glossaries, phrase books, etc., Dictionary [Publication Type], Frenulum linguae, Disposal, Automatic system recovery venue: Conference on Empirical Methods in Natural Language Processing year: 2003",
        "abstract": "We propose new methods to take advantage of text in resource-rich languages to sharpen statistical language models in resource-deficient languages. We achieve this through an extension of the method of lexical triggers to the cross-language problem, and by developing a likelihoodbased adaptation scheme for combining a trigger model with an -gram model. We describe the application of such language models for automatic speech recognition. By exploiting a side-corpus of contemporaneous English news articles for adapting a static Chinese language model to transcribe Mandarin news stories, we demonstrate significant reductions in both perplexity and recognition errors. We also compare our cross-lingual adaptation scheme to monolingual language model adaptation, and to an alternate method for exploiting cross-lingual cues, via crosslingual information retrieval and machine translation, proposed elsewhere."
    },
    "W03-1001": {
        "title": "A Projection Extension Algorithm for Statistical Machine Translation authors: C. Tillmann citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 4 isOpenAccess: True isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 82 numCiting: 12 s2FieldsOfStudy: Computer Science, Computer Science topics: Statistical machine translation, Algorithm, Language model, Trigram, Bitext word alignment, N-gram, Data structure alignment venue: Conference on Empirical Methods in Natural Language Processing year: 2003",
        "abstract": "In this paper, we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrasebased models. The units of translation are blocks \u2013 pairs of phrases. During decoding, we use a block unigram model and a word-based trigram language model. During training, the blocks are learned from source interval projections using an underlying high-precision word alignment. The system performance is significantly increased by applying a novel block extension algorithm using an additional highrecall word alignment. The blocks are further filtered using unigram-count selection criteria. The system has been successfully test on a Chinese-English and an ArabicEnglish translation task."
    },
    "S14-2072": {
        "title": "Meerkat Mafia: Multilingual and Cross-Level Semantic Textual Similarity Systems authors: Abhay Lokesh Kashyap, Lushan Han, Roberto Yus, Jennifer Sleeman, Taneeya Satyapanich, S. Gandhi, Timothy W. Finin citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 2 isOpenAccess: True isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 27 numCiting: 24 s2FieldsOfStudy: Computer Science, Computer Science, Linguistics topics: SemEval, Unsupervised learning, Semantic similarity, WordNet, Urban Dictionary, Word sense venue: International Workshop on Semantic Evaluation year: 2014",
        "abstract": "We describe UMBC\u2019s systems developed for the SemEval 2014 tasks on Multilingual Semantic Textual Similarity (Task 10) and Cross-Level Semantic Similarity (Task 3). Our best submission in the Multilingual task ranked second in both English and Spanish subtasks using an unsupervised approach. Our best systems for Cross-Level task ranked second in Paragraph-Sentence and first in both Sentence-Phrase and Word-Sense subtask. The system ranked first for the PhraseWord subtask but was not included in the official results due to a late submission."
    },
    "S14-2073": {
        "title": "MindLab-UNAL: Comparing Metamap and T-mapper for Medical Concept Extraction in SemEval 2014 Task 7 authors: Alejandro Riveros, Maria De-Arteaga, F. Gonzalez, Sergio Jimenez, H. Muller citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 0 isOpenAccess: True isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 2 numCiting: 5 s2FieldsOfStudy: Computer Science, Computer Science topics: SemEval, Principle of maximum entropy, Named-entity recognition, Dm-crypt, Unique identifier, Thesaurus venue: International Workshop on Semantic Evaluation year: 2014",
        "abstract": "This paper describes our participation in task 7 of SemEval 2014, which focuses on analysis of clinical text. The task is divided into two parts: recognizing mentions of concepts that belong to the UMLS (Unified Medical Language System) semantic group disorders, and mapping each disorder to a unique UMLS CUI (Concept Unique Identifier), if possible. For identifying and mapping disorders belonging to the UMLS meta thesaurus, we explore two tools: Metamap and T-mapper. Additionally, a Named Entity Recognition system, based on a maximum entropy model, was implemented to identify other disorders."
    },
    "S14-2070": {
        "title": "LT3: Sentiment Classification in User-Generated Content Using a Rich Feature Set authors: Cynthia Van Hee, M. Kauter, Orphee De Clercq, Els Lefever, Veronique Hoste citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 0 isOpenAccess: True isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 14 numCiting: 20 s2FieldsOfStudy: Computer Science, Computer Science topics: Sentiment analysis, User-generated content, Emoticon, Hashtag, N-gram, Experiment, Brown Corpus, Feature selection, Word-sense disambiguation, Lemmatisation, Lexicon, Grams, Modal logic, Viz: The Computer Game, Error analysis (mathematics) venue: International Workshop on Semantic Evaluation year: 2014",
        "abstract": "This paper describes our contribution to the SemEval-2014 Task 9 on sentiment analysis in Twitter. We participated in both strands of the task, viz. classification at message-level (subtask B), and polarity disambiguation of particular text spans within a message (subtask A). Our experiments with a variety of lexical and syntactic features show that our systems benefit from rich feature sets for sentiment analysis on user-generated content. Our systems ranked ninth among 27 and sixteenth among 50 submissions for task A and B respectively."
    },
    "S14-2071": {
        "title": "LyS: Porting a Twitter Sentiment Analysis Approach from Spanish to English authors: David Vilares, Miguel Hermo, Miguel A. Alonso, Carlos Gomez-Rodriguez, Yerai Doval citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 0 isOpenAccess: True isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 5 numCiting: 19 s2FieldsOfStudy: Computer Science, Computer Science topics: Sentiment analysis, SemEval, Information gain in decision trees, Brown Corpus, Olami-Feder-Christensen model, Point of sale, Preprocessor, Relevance, Sparse matrix, Lexicon, Bag-of-words model in computer vision, Hoc (programming language), Tass Times in Tonetown, Kullback-Leibler divergence, Experiment, Information retrieval, Machine learning venue: International Workshop on Semantic Evaluation year: 2014",
        "abstract": "This paper proposes an approach to solve message- and phrase-level polarity classification in Twitter, derived from an existing system designed for Spanish. As a first step, an ad-hoc preprocessing is performed. We then identify lexical, psychological and semantic features in order to capture different dimensions of the human language which are helpful to detect sentiment. These features are used to feed a supervised classifier after applying an information gain filter, to discriminate irrelevant features. The system is evaluated on the SemEval 2014 task 9: Sentiment Analysis in Twitter. Our approach worked competitively both in message- and phraselevel tasks. The results confirm the robustness of the approach, which performed well on different domains involving short informal texts."
    },
    "S14-2076": {
        "title": "NRC-Canada-2014: Detecting Aspects and Sentiment in Customer Reviews authors: S. Kiritchenko, Xiao-Dan Zhu, Colin Cherry, Saif M. Mohammad citationVelocity: 69 fieldsOfStudy: Computer Science influentialCitationCount: 73 isOpenAccess: True isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 565 numCiting: 22 s2FieldsOfStudy: Computer Science, Computer Science topics: Sensor, Sentiment analysis, Laptop, Lexicon, SemEval, N-gram venue: International Workshop on Semantic Evaluation year: 2014",
        "abstract": "Reviews depict sentiments of customers towards various aspects of a product or service. Some of these aspects can be grouped into coarser aspect categories. SemEval-2014 had a shared task (Task 4) on aspect-level sentiment analysis, with over 30 teams participated. In this paper, we describe our submissions, which stood first in detecting aspect categories, first in detecting sentiment towards aspect categories, third in detecting aspect terms, and first and second in detecting sentiment towards aspect terms in the laptop and restaurant domains, respectively."
    },
    "S14-2077": {
        "title": "NRC-Canada-2014: Recent Improvements in the Sentiment Analysis of Tweets authors: Xiao-Dan Zhu, S. Kiritchenko, Saif M. Mohammad citationVelocity: 7 fieldsOfStudy: Computer Science influentialCitationCount: 12 isOpenAccess: True isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 138 numCiting: 13 s2FieldsOfStudy: Computer Science, Computer Science topics: Sentiment analysis, Lexicon, Test set, Blog, Supervised learning, F1 score, Cross-validation (statistics) venue: International Workshop on Semantic Evaluation year: 2014",
        "abstract": "This paper describes state-of-the-art statistical systems for automatic sentiment analysis of tweets. In a Semeval-2014 shared task (Task 9), our submissions obtained highest scores in the term-level sentiment classification subtask on both the 2013 and 2014 tweets test sets. In the message-level sentiment classification task, our submissions obtained highest scores on the LiveJournal blog posts test set, sarcastic tweets test set, and the 2013 SMS test set. These systems build on our SemEval-2013 sentiment analysis systems (Mohammad et al., 2013) which ranked first in both the termand message-level subtasks in 2013. Key improvements over the 2013 systems are in the handling of negation. We create separate tweet-specific sentiment lexicons for terms in affirmative contexts and in negated contexts."
    },
    "S14-2074": {
        "title": "NILC USP: An Improved Hybrid System for Sentiment Analysis in Twitter Messages authors: P. B. Filho, L. Avanco, T. Pardo, M. G. V. Nunes citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 3 isOpenAccess: True isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 20 numCiting: 14 s2FieldsOfStudy: Computer Science, Computer Science topics: Sentiment analysis, Machine learning, SemEval, Hybrid system, Lexicon, Universal Storage Platform, Pipeline (computing), Logic programming venue: International Workshop on Semantic Evaluation year: 2014",
        "abstract": "This paper describes the NILC USP system that participated in SemEval-2014 Task 9: Sentiment Analysis in Twitter, a re-run of the SemEval 2013 task under the same name. Our system is an improved version of the system that participated in the 2013 task. This system adopts a hybrid classification process that uses three classification approaches: rule-based, lexiconbased and machine learning. We suggest a pipeline architecture that extracts the best characteristics from each classifier. In this work, we want to verify how this hybrid approach would improve with better classifiers. The improved system achieved an F-score of 65.39% in the Twitter message-level subtask for 2013 dataset (+ 9.08% of improvement) and 63.94% for 2014 dataset."
    },
    "S14-2075": {
        "title": "NILC USP: Aspect Extraction using Semantic Labels authors: P. B. Filho, T. Pardo citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 0 isOpenAccess: True isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 3 numCiting: 16 s2FieldsOfStudy: Computer Science, Computer Science topics: Conditional random field, Sentiment analysis, SemEval, Machine learning, Universal Storage Platform, Algorithm venue: International Workshop on Semantic Evaluation year: 2014",
        "abstract": "This paper details the system NILC USP that participated in the Semeval 2014: Aspect Based Sentiment Analysis task. This system uses a Conditional Random Field (CRF) algorithm for extracting the aspects mentioned in the text. Our work added semantic labels into a basic feature set for measuring the efficiency of those for aspect extraction. We used the semantic roles and the highest verb frame as features for the machine learning. Overall, our results demonstrated that the system could not improve with the use of this semantic information, but its precision was increased."
    },
    "W10-4159": {
        "title": "Research of People Disambiguation by Combining Multiple knowledges authors: Erlei Ma, Yuanchao Liu citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 0 isOpenAccess: False isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 0 numCiting: 9 s2FieldsOfStudy: Computer Science, Computer Science topics: Word-sense disambiguation, Internet, Web search engine, Information source venue: CIPS-SIGHAN year: 2010",
        "abstract": "With the rapid development of Internet and many related technology, Web has become the main source of information. For many search engines, there are many different identities in the returned results of character information query. Thus the Research of People disambiguation is important. In this paper we attempt to solve this problem by combing different knowledge. As people usually have different kind of careers, so we first utilize this knowledge to classify people roughly. Then we use social context of people to identify different person. The experimental results show that these knowledge are helpful for people disambiguation."
    },
    "S13-2008": {
        "title": "HsH: Estimating Semantic Similarity of Words and Short Phrases with Frequency Normalized Distance Measures authors: Christian Wartena citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 1 isOpenAccess: False isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 9 numCiting: 19 s2FieldsOfStudy: Computer Science, Computer Science topics: SemEval, Semantic similarity, Support vector machine, Similarity measure, Hoc (programming language), Jackson system development, Shannon (unit), Jensen's inequality venue: International Workshop on Semantic Evaluation year: 2013",
        "abstract": "This paper describes the approach of the Hochschule Hannover to the SemEval 2013 Task Evaluating Phrasal Semantics. In order to compare a single word with a two word phrase we compute various distributional similarities, among which a new similarity measure, based on Jensen-Shannon Divergence with a correction for frequency effects. The classification is done by a support vector machine that uses all similarities as features. The approach turned out to be the most successful one in the task."
    },
    "S12-1063": {
        "title": "UTDHLT: COPACETIC System for Choosing Plausible Alternatives authors: Travis R. Goodwin, Bryan Rink, Kirk Roberts, S. Harabagiu citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 1 isOpenAccess: False isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 15 numCiting: 23 s2FieldsOfStudy: Computer Science, Computer Science topics: Bigram, TimeML, Commonsense knowledge (artificial intelligence), Uniform theory of diffraction, Causal filter, SemEval, Causality, Commonsense reasoning venue: International Workshop on Semantic Evaluation year: 2012",
        "abstract": "The Choice of Plausible Alternatives (COPA) task in SemEval-2012 presents a series of forced-choice questions wherein each question provides a premise and two viable cause or effect scenarios. The correct answer is the cause or effect that is the most plausible. This paper describes the COPACETIC system developed by the University of Texas at Dallas (UTD) for this task. We approach this task by casting it as a classification problem and using features derived from bigram co-occurrences, TimeML temporal links between events, single-word polarities from the Harvard General Inquirer, and causal syntactic dependency structures within the gigaword corpus. Additionally, we show that although each of these components improves our score for this evaluation, the difference in accuracy between using all of these features and using bigram co-occurrence information alone is not statistically significant."
    },
    "W09-2417": {
        "title": "SemEval-2010 Task 10: Linking Events and Their Participants in Discourse Josef Ruppenhofer and Caroline Sporleder Roser Morante authors: Josef Ruppenhofer, C. Sporleder, Roser Morante, Collin F. Baker, Martha Palmer citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 0 isOpenAccess: False isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 25 numCiting: 11 s2FieldsOfStudy: Computer Science, Computer Science, Psychology topics:  venue: International Workshop on Semantic Evaluation year: 2010",
        "abstract": "In this paper, we describe the SemEval-2010 shared task on \u201cLinking Events and Their Participants in Discourse\u201d. This task is a variant of the classical semantic role labelling task. The novel aspect is that we focus on linking local semantic argument structures across sentence boundaries. Specifically, the task aims at linking locally uninstantiated roles to their coreferents in the wider discourse context (if such co-referents exist). This task is potentially beneficial for a number of NLP applications and we hope that it will not only attract researchers from the semantic role labelling community but also from co-reference resolution and information extraction."
    },
    "W95-0111": {
        "title": "Automatic Suggestion of Significant Terms for a Predefined authors: Joe F. Zhou, Pete Dapkus citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 0 isOpenAccess: False isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 11 numCiting: 13 s2FieldsOfStudy: Computer Science, Computer Science, Business topics: Mutual information, Subject-matter expert, Spelling suggestion, Algorithm, Computation, Statistical model venue: VLC@ACL year: 1995",
        "abstract": "This paper presents a preliminary experiment in automatically suggesting significant terms for a predefined topic. The general method is to compare a topically focused sample created around the predefined topic with a larger and more general base sample. A set of statistical measures are used to identify significant word units in both samples. Identification of single word terms is based on the notion of word intervals. Two-word terms are identified through the computation of mutual information, and the extension of mutual information assists in capturing multi-word terms. Once significant terms of all these three types are identified, a comparison algorithm is applied to differentiate terms across the two data samples. If significant changes in the values of certain statistical variables are detected, associated terms will selected as being topic-oriented and included in a suggested list. To check the quality of the suggested terms, we compare them against terms manually determined by the domain expert. Though overlaps vary, we find that the automatical suggestion provides more terms that are useful for describing the predefined topic."
    },
    "W14-3200": {
        "title": "Workshop on Computational Linguistics and Clinical Psychology From Linguistic Signal to Clinical Reality Proceedings of the Workshop authors: Kristy Hollingshead, Molly Ireland, Kate Loveys citationVelocity: 0 fieldsOfStudy: Psychology influentialCitationCount: 2 isOpenAccess: True isPublisherLicensed: False is_publisher_licensed: False numCitedBy: 28 numCiting: 0 s2FieldsOfStudy: Psychology, Linguistics topics: Computational linguistics, Computation venue:  year: 2017",
        "abstract": "Mental health problems are among the costliest challenges we face, in every possible sense of cost. The numbers are staggering: to cite just a few, in the United States mental health spending accounted for $33 billion in 1986, $100 billion in 2003, and is projected to increase to $203 billion for 2014; some 25 million American adults will have an episode of major depression this year; and suicide is the third leading cause of death for people between 10 and 24 years old. The importance of clinical psychology as a problem space cannot be overstated. For clinical psychologists, language plays a central role in diagnosis. Indeed, many clinical instruments fundamentally rely on what is, in effect, manual annotation of patient language. Applying language technology in this domain, e.g. in language-based assessment, could potentially have an enormous impact, because many individuals are motivated to underreport psychiatric symptoms (consider active duty soldiers, for example) or lack the self-awareness to report accurately (consider individuals involved in substance abuse who do not recognize their own addiction), and because many people \u2014 e.g. those without adequate insurance or in rural areas \u2014 cannot even obtain access to a clinician who is qualified to perform a psychological evaluation. Bringing language technology to bear on these problems could potentially lead to inexpensive screening measures that could be administered by a wider array of healthcare professionals, which is particularly important since the majority of individuals who present with symptoms of mental health problems do so in a primary care physician\u2019s office. Given the burden on primary care physicians to diagnose mental health disorders in very little time, the American Academy of Family Physicians has recognized the need for diagnostic tools for physicians that are \u201csuited to the realities of their practice&quot;. Although automated language analysis connected with mental health conditions goes back at least as far as the 1990s, it has not been a major focus for computational linguistics compared with other application domains. However, recently there has been noticable uptick in research activity on this topic. One recent shared task brings together research on the Big-5 personality traits, and another involved research on identification of emotion in suicide notes. Research has been done on language analysis in the context of, for example, autistic spectrum disorders, dementia, depression, post-partum depression, general life satisfaction , and suicide risk. This increase in attention is consistent with, and gains power from, the recent rise in computational linguistics activity connected with computational social science more broadly. With computational linguistics research on this topic moving toward critical mass, one key goal of this workshop was to bring together researchers to discuss the current state of the art, share methods, and set directions for the future. The workshop had a second goal also, though: to directly engage clinical practitioners in mental health. By including clinicians on our program committee and as discussants, the workshop was designed to increase NLP practitioners\u2019 understanding of what mental health clinicians do and what their real needs are, and to increase clinical practitioners\u2019 understanding of what is possible in NLP and what it might have to offer. We received a total of 17 submissions. Of these, 7 (41%) were accepted for oral presentation and discussion, and an additional 7 were selected for inclusion in the workshop\u2019s poster session. We wish to thank everyone who showed interest and submitted a paper, all of the authors for their iii contributions, the members of the Program Committee for their thoughtful reviews, our clinical practitioner discussants, and all the attendees of the workshop. We also wish to extend sincere thanks to the Association for Computational Linguistics for making this workshop possible, and to CHIB, the Center for Health-related Informatics and Bioimaging at the University of Maryland, for its generous sponsorship. Applying prosodic speech features in mental health care: An exploratory study in a lifereview intervention for depression"
    },
    "W09-2414": {
        "title": "SemEval-2010 Task 7: Argument Selection and Coercion authors: J. Pustejovsky, Anna Rumshisky, Alex Plotnick, Elisabetta Jezek, O. Batiukova, Valeria Quochi citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 2 isOpenAccess: True isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 16 numCiting: 46 s2FieldsOfStudy: Computer Science, Linguistics topics: SemEval, Specification language, Predicate (mathematical logic), Requirement venue: SemEval@ACL year: 2009",
        "abstract": "In this paper, we describe the Argument Selection and Coercion task, currently in development for the SemEval-2 evaluation exercise scheduled for 2010. This task involves characterizing the type of compositional operation that exists between a predicate and the arguments it selects. Specifically, the goal is to identify whether the type that a verb selects is satisfied directly by the argument, or whether the argument must change type to satisfy the verb typing. We discuss the problem in detail and describe the data preparation for the task."
    },
    "J02-1002": {
        "title": "A Critique and Improvement of an Evaluation Metric for Text Segmentation authors: L. Pevzner, Marti A. Hearst citationVelocity: 19 fieldsOfStudy: Computer Science influentialCitationCount: 45 isOpenAccess: False isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 461 numCiting: 43 s2FieldsOfStudy: Computer Science, Computer Science topics: Text segmentation, Algorithm venue: International Conference on Computational Logic year: 2002",
        "abstract": "Text segmentation is the task of determining the positions at which topics change in a stream of text. Interest in automatic text segmentation has blossomed over the last few years, with applications ranging from information retrieval to text summarization to story segmentation of video feeds. Early work in multiparagraph discourse segmentation examined the problem of subdividing texts into multiparagraph units that represent passages or subtopics. An example, drawn from Hearst (1997), is a 21paragraph science news article, called \u201cStargazers,\u201d whose main topic is the existence of life on earth and other planets. Its contents can be described as consisting of the following subtopic discussions (numbers indicate paragraphs): The TextTiling algorithm (Hearst 1993, 1994, 1997) attempts to recognize these subtopic changes by making use of patterns of lexical co-occurrence and distribution; subtopic boundaries are assumed to occur at the point in the documents at which large shifts in vocabulary occur. Many others have used this technique, or slight variations of it, for subtopic segmentation (Nomoto and Nitta 1994; Hasnah 1996; Richmond, Smith, and Amitay 1997; Heinonen 1998; Boguraev and Neff 2000). Other techniques use clustering and/or similarity matrices based on word co-occurrences (Reynar 1994; Yaari 1997; Choi 2000), and still others use machine learning techniques to detect cue words, or hand-selected cue words to detect segment boundaries (Passonneau and Litman 1993; Beeferman, Berger, and Lafferty 1997; Manning 1998). Researchers have explored the use of this kind of document segmentation to improve automated summarization (Salton et al. 1994; Barzilay and Elhadad 1997; Kan, Klavans, and McKeown 1998; Mittal et al. 1999; Boguraev and Neff 2000) and automated genre detection (Karlgren 1996). Text segmentation issues are also important for passage retrieval, a subproblem of information retrieval (Hearst and Plaunt 1993; Salton, Allan, and Buckley 1993; Callan 1994; Kaszkiel and Zobel 1997). More recently, a great deal of interest has arisen in using automatic segmentation for the detection of topic and story boundaries in news feeds (Mani et al. 1997; Merlino, Morey, and Maybury 1997; Ponte and Croft 1997; Hauptmann and Witbrock 1998; Allan et al. 1998; Beeferman, Berger, and Lafferty 1997, 1999). Sometimes segmentation is done at the clause level, for the purposes of detecting nuances of dialogue structure or for more sophisticated discourse-processing purposes (Morris and Hirst 1991; Passonneau and Litman 1993; Litman and Passonneau 1995; Hirschberg and Nakatani 1996; Marcu 2000). Some of these algorithms produce hierarchical dialogue segmentations whose evaluation is outside the scope of this discussion. There are two major difficulties associated with evaluating algorithms for text segmentation. The first is that since human judges do not always agree where boundaries should be placed and how fine grained an analysis should be, it is difficult to choose a reference segmentation for comparison. Some evaluations circumvent this difficulty by detecting boundaries in sets of concatenated documents, where there can be no disagreements about the fact of the matter (Reynar 1994; Choi 2000); others have several human judges make ratings to produce a \u201cgold standard.\u201d The second difficulty with evaluating these algorithms is that for different applications of text segmentation, different kinds of errors become important. For instance, for information retrieval, it can be acceptable for boundaries to be off by a few sentences\u2014 a condition called a near miss\u2014but for news boundary detection, accurate placement is crucial. For this reason, some researchers prefer not to measure the segmentation algorithm directly, but consider its impact on the end application (Manning 1998; Kan, Klavans, and McKeown 1998). Our approach to these two difficulties is to evaluate algorithms on real segmentations using a \u201cgold standard\u201d and to develop an evaluation algorithm that suits all applications reasonably well. Precision and recall are standard evaluation measures for information retrieval tasks and are often applied to evaluation of text segmentation algorithms as well. Precision is the percentage of boundaries identified by an algorithm that are indeed true boundaries; recall is the percentage of true boundaries that are identified by the algorithm. However, precision and recall are problematic for two reasons. The first is that there is an inherent trade-off between precision and recall; improving one tends to cause the score for the other to decline. In the segmentation example, positing more boundaries will tend to improve the recall but at the same time reduce the precision. Some evaluators use a weighted combination of the two known as the F-measure (Baeza-Yates and Ribeiro-Neto 1999), but this is difficult to interpret (Beeferman, Berger, and Lafferty 1999). Another approach is to plot a precision-recall curve, showing the scores for precision at different levels of recall. Two hypothetical segmentations of the same reference (ground truth) document segmentation. The boxes indicate sentences or other units of subdivision, and spaces between boxes indicate potential boundary locations. Algorithm A-0 makes two near misses, while Algorithm A-1 misses both boundaries by a wide margin and introduces three false positives. Both algorithms would receive scores of 0 for both precision and recall. Another problem with precision and recall is that they are not sensitive to near misses. Consider, for example, a reference segmentation and the results obtained by two different text segmentation algorithms, as depicted in Figure 1. In both cases, the algorithms fail to match any boundary precisely; both receive scores of 0 for precision and recall. However, Algorithm A-0 is close to correct in almost all cases, whereas Algorithm A-1 is entirely off, adding extraneous boundaries and missing important boundaries entirely. In some circumstances, it would be useful to have an evaluation metric that penalizes A-0 less harshly than A-1. Beeferman, Berger, and Lafferty (1997) introduce a new evaluation metric that attempts to resolve the problems with precision and recall, including assigning partial credit to near misses. They justify their metric as follows (page 43): Segmentation ... is about identifying boundaries between successive units of information in a text corpus. Two such units are either related or unrelated by the intent of the document author. A natural way to reason about developing a segmentation algorithm is therefore to optimize the likelihood that two such units are correctly labeled as being related or being unrelated. Our error metric P\u00b5 is simply the probability that two sentences drawn randomly from the corpus are correctly identified as belonging to the same document or not belonging to the same document. The derivation of P\u00b5 is rather involved, and a much simpler version is adopted in the later work (Beeferman, Berger, and Lafferty 1999) and by others. This version, referred to as Pk, is calculated by setting k to half of the average true segment size and then computing penalties via a moving window of length k. At each location, the algorithm determines whether the two ends of the probe are in the same or different segments in the reference segmentation and increases a counter if the algorithm\u2019s segmentation disagrees. The resulting count is scaled between 0 and 1 by dividing by the number of measurements taken. An algorithm that assigns all boundaries correctly receives a score of 0. Beeferman, Berger, and Lafferty (1999) state as part of An illustration of how the Pk metric handles false negatives. The arrowed lines indicate the two poles of the probe as it moves from left to right; the boxes indicate sentences or other units of subdivision; and the width of the window (k) is four, meaning four potential boundaries fall between the two ends of the probe. Solid lines indicate no penalty is assigned; dashed lines indicate a penalty is assigned. Total penalty is always k for false negatives. the justification for this metric that, to discourage \u201ccheating\u201d of the metric, degenerate algorithms\u2014those that place boundaries at every position, or place no boundaries at all\u2014are assigned (approximately) the same score. Additionally, the authors define a false negative (also referred to as a miss) as a case when a boundary is present in the reference segmentation but missing in the algorithm\u2019s hypothesized segmentation, and a false positive as an assignment of a boundary that does not exist in the reference segmentation. The Pk metric is fast becoming the standard among researchers working in text segmentation (Allan et al. 1998; Dharanipragada et al. 1999; Eichmann et al. 1999; van Mulbregt et al. 1999; Choi 2000). However, we have reservations about this metric. We claim that the fundamental premise behind it is flawed; additionally, it has several significant drawbacks, which we identify in this section. In the remainder of the paper, we suggest modifications to resolve these problems, and we report the results of simulations that validate the analysis and suggest that the modified metric is an improvement over the original. Assume a text with segments of average size 2k, where k is the distance between the two ends of the Pk probe. If the algorithm misses a boundary\u2014produces a false negative\u2014it receives k penalties. To see why, suppose S1 and S2 are two segments of length 2k, and the algorithm misses the transition from S1 to S2. When Pk sweeps across S1, if both ends of the probe point to sentences that are inside S1, the two sentences are in the same segment in both the reference and the hypothesis, and no penalty is incurred. When the right end of the probe crosses the reference boundary between S1 and S2, it will start recording nonmatches, since the algorithm assigns the two sentences to the same segment, while the reference does not. This circumstance happens k times, until both ends of the probe point to sentences that are inside S2. (See Figure 2.) This analysis assumes average size segments; variation in segment size is discussed below, but does not have a large effect on this result. An illustration of how the Pk metric handles false positives. Notation is as in Figure 2. Total penalty depends on the distance between the false positive and the relevant correct boundaries; on average, it is k2, assuming a uniform distribution of boundaries across the document. This example shows the consequences of two different locations of false positives: on the left, the penalty is k2; on the right, it is k. Now, consider false positives. A false positive occurs when the algorithm places a boundary at some position where there is no boundary in the reference segmentation. The number of times that this false positive is noted by Pk depends on where exactly inside S2 the false positive occurs. (See Figure 3.) If it occurs in the middle of the segment, the false positive is noted k times (as seen on the right-hand side of Figure 3). If it occurs j < k sentences from the beginning or the end of the segment, the segmentation is penalized j times. Assuming uniformly distributed false positives, on average a false positive is noted 2k times by the metric\u2014half the rate for false negatives. This average increases with segment size, as we will discuss later, and changes if one assumes different distributions of false positives throughout the document. However, this does not change the fact that in most cases, false positives are penalized some amount less than false negatives. This is not an entirely undesirable side effect. This metric was devised to take into account how close an assigned boundary is to the true one, rather than just marking it as correct or incorrect. This method of penalizing false positives achieves this goal: the closer the algorithm\u2019s boundary is to the actual boundary, the less it is penalized. However, overpenalizing false negatives to do this is not desirable. One way to fix the problem of penalizing false negatives more than false positives is to double the false positive penalty (or halve the false negative penalty). However, this would undermine the probabilistic nature of the metric. In addition, doubling the penalty may not always be the correct solution, since segment size will vary from the average, and false positives are not necessarily uniformly distributed throughout the document. Another important problem with the Pk metric is that it allows some errors to go unpenalized. In particular, it does not take into account the number of segment boundaries between the two ends of the probe. (See Figure 4.) Let r; indicate the number of boundaries between the ends of the probe according to the reference segmentation, and let a; indicate the number of boundaries proposed by some text segmentation algorithm for the same stretch of text. If r; = 1 (the reference segmentation indicates one boundary) and a; = 2 (the algorithm marks two boundaries within this range), then the algorithm makes at least one false positive (spurious boundary) error. However, the evaluation metric Pk does not assign a penalty in this situation. Similarly, if r; = 2 and a; = 1, the An illustration of the fact that the Pk metric fails to penalize false positives that fall within k sentences of a true boundary. Notation is as in Figure 2. algorithm has made at least one false negative (missing boundary) error, but it is not penalized for this error under Pk. The size of the segment plays a role in the amount that a false positive within the segment or a false negative at its boundary is penalized. Let us consider false negatives (missing boundaries) first. As seen above, with average size segments, the penalty for a false negative is k. For larger segments, it remains at k\u2014it cannot be any larger than that, since for a given position i there can be at most k intervals of length k that include that position. As segment size gets smaller, however, the false negative penalty changes. Suppose we have two segments, A and B, and the algorithm misses the boundary between them. Then the algorithm will be penalized k times if Size(A)+ Size(B) > 2k, that is, as long as each segment is about half the average size or larger. The penalty will then decrease linearly with Size(A)+Size(B) so long as k < Size(A)+ Size(B) < 2k. To be more exact, the penalty actually decreases linearly as the size of either segment decreases below k. This is intuitively clear from the simple observation that in order to incur a penalty at any range ri for a false negative, it has to be the case that ri > ai. In order for this to be true, both the segment to the left and the segment to the right of the missed boundary have to be of size greater than k; otherwise, the penalty can only be equal to the size of the smaller segment. When Size(A)+Size(B) < k, the penalty disappears completely, since then the probe\u2019s interval is larger than the combined size of both segments, making it not sensitive enough to detect the false negative. It should be noted that fixing Problem 2 would at least partially fix this bias as well. Now, consider false positives (extraneous boundaries). For average segment size and a uniform distribution of false positives, the average penalty is k2, as described earlier. In general, in large enough segments, the penalty when the false positive is a distance d < k from a boundary is d, and the penalty when the false positive is a distance d > k from a boundary is k. Thus, for larger segments, the average penalty assuming a uniform distribution becomes larger, because there are more places in the segment that are at least k positions away from a boundary. The behavior at the edges of the segments remains the same, though, so the average penalty never reaches k. Now, consider what happens with smaller segments. Suppose we have a false positive in Segment A. As Size(A) decreases from 2k to k, the average false positive penalty decreases linearly with it, because when Size(A) decreases below 2k, the maximum distance any sentence can be from a boundary becomes less than k. Therefore, the A reference segmentation and five different hypothesized segmentations with different properties. maximum possible penalty for a false positive in A is less than k, and this number continues to decrease as Size(A) decreases. When Size(A) < k, the false positive penalty disappears, for the same reason that the false negative penalty disappears for smaller segments. Again, fixing Problem 2 would go a long way toward eliminating this bias. Thus, errors in larger-than-average segments increase the penalty slightly (for false positives) or not at all (for false negatives) as compared to average size segments, while errors in smaller-than-average segments decrease the penalty significantly for both types of error. This means that as the variation of segment size increases, the metric becomes more lenient, since it severely underpenalizes errors in smaller segments, while not making up for this by overpenalizing errors in larger segments. Reconsider the segmentation made by Algorithm A-0 in Figure 1. In both cases of boundary assignment, Algorithm A-0 makes both a false positive and a false negative error, but places the boundary very close to the actual one. We will call this kind of error a near-miss error, distinct from a false positive or false negative error. Distinguishing this type of error from \u201cpure\u201d false positives better reflects the goal of creating a metric different from precision and recall, since it can be penalized less than a false negative or a false positive. Now, consider the algorithm segmentations shown in Figure 5. Each of the five algorithms makes a mistake either on the boundary between the first and second segment of the reference segmentation, or within the second segment. How should these various segmentations be penalized? In the analysis below, we assume an application for which it is important not to introduce spurious boundaries. These comparisons will most likely vary depending on the goals of the target application. Algorithm A-4 is arguably the worst of the examples, since it has a false positive and a false negative simultaneously. Algorithms A-0 and A-2 follow: they contain a pure false negative and false positive, respectively. Comparing Algorithms A-1 and A-3, we see that Algorithm A-3 is arguably better, because it recognizes that only one boundary is present rather than two. Algorithm A-1 does not recognize this, and inserts an extra segment. Even though Algorithm A-1 actually places a correct boundary, it also places an erroneous boundary, which, although close to the actual one, is still a false positive\u2014in fact, a pure false positive. For this reason, Algorithm A-3 can be considered better than Algorithm A-1. Now, consider how Pk treats the five types of mistakes above. Again, assume the first and second segments in the reference segmentation are average size segments. Algorithm A-4 is penalized the most, as it should be. The penalty is as much as 2k if the false positive falls in the middle of Segment C, and it is > k as long as the false positive is a distance > 2k from the actual boundary between the first and second reference segments. The penalty is large because the metric catches both the false negative and the false positive errors. The segmentations assigned by Algorithms A-0 and A-2 are treated as discussed earlier in conjunction with Problem 1: the one assigned by Algorithm A-0 has a false negative and thus incurs a penalty of k, and the one assigned by Algorithm A-2 has a false positive, and thus incurs a penalty of < k. Finally, consider the segmentations assigned by Algorithms A-1 and A-3, and suppose that both contain an incorrect boundary some small distance e from the actual one. Then the penalty for Algorithm A-1 is e, while the penalty for Algorithm A-3 is 2e. This should not be the case; Algorithm A-1 should be penalized more than Algorithm A-3, since a near-miss error is better than a pure false positive, even if it is close to the boundary. Pk is nonintuitive because it measures the probability that two sentences k units apart are incorrectly labeled as being in different segments, rather than directly reflecting the competence of the algorithm. Although perfect algorithms score 0, and various degenerate ones score 0.5, numerical interpretation and comparison are difficult because it is not clear how the scores are scaled."
    },
    "J02-1003": {
        "title": "Generating Referring Expressions: Boolean Extensions of the Incremental Algorithm authors: Kees van Deemter citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 12 isOpenAccess: False isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 130 numCiting: 36 s2FieldsOfStudy: Computer Science, Computer Science topics: Algorithm, Boolean expression venue: Computational Linguistics year: 2002",
        "abstract": "Generation of referring expressions (GRE) is a key task of most natural language generation (NLG) systems (e.g., Reiter and Dale 2000, Section 5.4). Regardless of the type of knowledge base (KB) forming the input to the generator, many objects will not be designated in it via an ordinary proper name. A person like Mr. Jones, for example, may be designated using an artificial name like #Jones083, if the name Jones is not uniquely distinguishing. The same is true for a piece of furniture, a tree, or an atomic particle, for instance, for which no proper name is in common use at all, or (in most cases) if the generator tries to refer to an entire set of objects. In all such cases, the generator has to \u201cinvent\u201d a description that enables the hearer to identify the intended referent. In the case of Mr. Jones, for example, the program could identify him by providing his full name and address; in the case of a tree, some longer description may be necessary. Henceforth, we will call the intended referent the target of the GRE algorithm. The question that we set out to answer is whether existing GRE algorithms produce adequate descriptions whenever such descriptions exist: in short, whether these algorithms are, as we shall say, complete. The paper brings a degree of formal precision to this issue and reveals a number of reasons why current GRE algorithms are incomplete; we sketch remedies and discuss their consequences in terms of linguistic coverage and computational tractability. We take the Incremental Algorithm (Dale and Reiter 1995) to represent the state of the art in this area, and we minimize the deviations from this algorithm. As a result, this paper might be read as an investigation into how widely the ideas underlying the Incremental Algorithm can be used, and the extent to which they may be generalized. The main generalization that we will investigate involves complex Boolean combinations of properties, that is, descriptions that involve more than a merely intersective (i.e., logically conjunctive) combination of properties. Such generalizations are natural because the properties involved are implicitly present in the KB, as we will explain; they become especially relevant when the algorithms are also generalized to generate references to sets, rather than individual objects. But, before we arrive at these generalizations, we will identify and confront a number of cases in which current GRE algorithms are incomplete even with respect to merely intersective descriptions. In this paper, we will deal with \u201cfirst mention\u201d descriptions only (unlike Dale 1992, Chapter 5; Mittal et al. 1998; Kibble 1999), assuming that the information used for generating the description is limited to a KB containing complete information about which properties are true of each object. Also, we focus on \u201cone shot\u201d descriptions, disregarding cases where an object is described through its relations with other objects (Dale and Haddock 1991; Horacek 1997; Krahmer, van Erk, and Verleg 2001). More crucially, we follow Dale and Reiter (1995) in focusing on the semantic content of a description (i.e., the problem of content determination, for short), assuming that any combination of properties can be expressed by the NLG module responsible for linguistic realization. This modular approach allows us to separate logical aspects of generation (which are largely language independent) from purely linguistic aspects, and it allows the realization module to base its decisions on complete information about which combination of properties is to be realized. Accordingly, when we write Generation of Referring Expressions or GRE, we will refer specifically to determination of the semantic content of a description. Analogously, the word description will refer to the semantic content of a linguistic expression only. Note that our modular approach makes it unnatural to assume that a description is always expressed by a single noun phrase: if several sentences are needed, then so be it. After summarizing the Incremental Algorithm in Section 2, in Section 3 we take a closer look at the algorithm in its standard, \u201cintersective\u201d form, in which it identifies an object by intersecting a number of atomic properties. We discuss cases in which this algorithm fails to find an adequate description even though such a description exists, and we propose a number of possible remedies. Having extablished a completeness result for a version of the intersective Incremental Algorithm, we turn to questions of completeness that involve more complex Boolean combinations in Section 4. In Section 5, we summarize the main results of our exploration and put them in perspective."
    },
    "J02-1001": {
        "title": "Binding Machines authors: A. Branco citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 0 isOpenAccess: False isPublisherLicensed: True is_publisher_licensed: True numCitedBy: 14 numCiting: 39 s2FieldsOfStudy: Computer Science, Computer Science topics: Design rationale, Anaphora (linguistics), Lisp machine, Relevance, Unification (computer science) venue: Computational Linguistics year: 2002",
        "abstract": "Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Nlitkov 1997, 1998), it has been common wisdom that factors determining the antecedents of anaphors divide into filters and preferences. The former exclude impossible antecedents and help to circumscribe the set of antecedent candidates; the latter help to pick the most likely candidate, which will be proposed as the antecedent. Binding constraints are a significant subset of such filters. As they delimit the relative positioning of anaphors and their possible antecedents in grammatical geometry, these constraints are crucial to restricting the search space for antecedents and enhancing the performance of anaphor resolvers.1 From an empirical perspective, they stem from quite robust generalizations and exhibit a universal character, given their parameterized validity across natural languages. From a conceptual point of view, in turn, the relations among binding constraints involve nontrivial symmetry, which lends them a modular nature. Accordingly, they have been considered one of the most robust and intriguing grammar submodules, usually referred to as binding theory. However, in contrast to this, the formal and computational handling of binding constraints has presented considerable resistance. Anaphor resolution typically builds on many sources of information\u2014among them, information about the grammatical structure of the sentence\u2014so that the different filters and preferences may be used. Consequently, it must in general be regarded as a postgrammatical process, in the sense that it is completed after sentences are parsed. Binding constraints, as a subset of the filters for anaphor resolution, are a special case in this respect. Given that they form a submodule of grammar, they are specified on a par with other grammatical submodules and constraints, and they are thus expected to be integrated already into the processing of grammar. Nevertheless, this integration cannot be considered to have been adequately achieved. As we will discuss at length, the original methodology for verifying the compliance of grammatical representations with binding constraints requires extragrammatical processing steps delivering a forest of indexed trees to anaphor resolvers (Chomsky 1981). More recently, constraint-based grammatical frameworks either require specialpurpose extensions of the description formalism, though ensuring only a partial handling of these constraints, as in Lexical-Functional Grammar (LFG; Dalrymple 1993), or do not offer a solution yet to integrate them into grammar, as in Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag 1994).2 Our primary goal here is thus to bridge the gap between the grammatical nature of binding constraints and their full integration into grammar processing. In particular, we aim at achieving this in such a way that a lean interface between grammar and reference processing emerges. In Section 2, we first underline the distinction, seldom taken into account, between specification and verification of binding constraints. We then review advances proposed in the literature concerning the completion of the verification task. We observe that three major lines of progress can be identified: packing of anaphoric ambiguity, packing of nonlocal context, and lexicalization of binding constraints. Building on these contributions, in Section 3 we argue that the remaining step forward is to harmonize these different advances. We suggest that a more accurate, semantics-driven comprehension of the nature of binding constraints is a relevant move toward this harmonization. On the basis of this revision, we introduce a methodology for verifying these constraints, which rests on the new concept of binding machine, to be defined. In Section 4, in the light of this new methodology, we show how binding constraints can be given a unification-based specification and can be fully integrated into grammar. In Section 5, we present an illustrative example and discuss in detail how binding constraints and reference-processing systems are coordinated, and how the previously identified drawbacks are overcome."
    },
    "J02-1006": {
        "title": "Book Reviews Tree Adjoining Grammars: Formalisms, Linguistic Analysis and Processing authors: K. PullumGeoffrey citationVelocity: 0 fieldsOfStudy: Computer Science influentialCitationCount: 0 isOpenAccess: False isPublisherLicensed: False is_publisher_licensed: False numCitedBy: 0 numCiting: 0 s2FieldsOfStudy: Computer Science, Linguistics topics: Tree-adjoining grammar venue:  year: 2002",
        "abstract": "Geoffrey K. Pullum is professor of linguistics at the University of California, Santa Cruz, where his teaching ranges from a linguistics graduate course in mathematical foundations of linguistics to a computer science freshman course on the Unix operating system. He is coauthor with Rodney Huddleston of a forthcoming book entitled The Cambridge Grammar of the English Language (Cambridge University Press). Pullum\u2019s e-mail address is pullum@ling.ucsc.edu; URL: http://ling.ucsc.edu/\ue187pullum. Marcu\u2019s monograph is based on his Ph.D. thesis\u2014research carried out at the Department of Computer Science, University of Toronto\u2014and subsequent work conducted at the Information Sciences Institute, University of Southern California. It argues for the idea that discourse/rhetorical relations that connect text spans of various length can be computed without a complete semantic analysis of sentences that make up these text segments. As an alternative, a formal specification of admissible text structures is provided, which constrains the range of possible semantic and functional connections between text spans and imposes strict well-formedness conditions on valid discourse structures. For effectively computing these text structures, mainly surface-oriented lexical cues and shallow text-parsing techniques are used. Complementary to these formal and computational considerations, Marcu reports on various evaluations, both intrinsic and extrinsic, in order to assess the strengths and weaknesses of his approach and the generality of the principles it is based on. These experiments were mostly carried out on Scientific American, TREC, MUC, Wall Street Journal, and Brown corpora. The book consists of three main parts. In the first part, linguistic and formal properties of coherent texts are discussed, with a focus on high-level discourse structures. This theoretical framework serves, in the second part, as the background for developing discourse structure parsing algorithms that compute rhetorical relations in realworld free texts. The benefits of such algorithms for building a high-performance text summarization system are dealt with in the third part. In the first part, the author factors out a set of assumptions that are common to prominent approaches to discourse structure. So, consensus has been reached that texts can be segmented into nonoverlapping, elementary textual units, that discourse relations of different types link (elementary and complex) textual units of various sizes, that some textual units are more important to the writer\u2019s communicative intentions and goals than others, and that trees are a good approximation of the abstract structure of most texts. These considerations lead to a compositionality criterion that requires that discourse relations that link two large text spans can be explained by discourse relations that hold between at least two of the most salient text units of the constituent spans. This notion then forms the basis for a first-order logic axiomatization that captures formal properties of valid text structures. Although this formalization is independent of the set of rhetorical relations actually considered, it yields, by proper relation instantiation, a formal characterization of the structural properties that are specific to Rhetorical Structure Theory (RST) (Mann and Thompson 1988). Building on these formal considerations, the author discusses three (nonincremental) algorithmic paradigms that compute some or all valid discourse structures of a text. Two of them employ model-theoretic techniques and encode the problem of text-structure derivation as a classical constraint satisfaction problem and as a propositional satisfiability problem. The other one is grammar-based and builds on a proof theory for solving the text-structure derivation problem (demonstrated to be sound and complete with respect to the given logical formalization). The performance of these algorithms is compared empirically on a benchmark of eight manually encoded text-structure derivation problems. Marcu uses logic to distinguish between discourse structures that are valid and those that are not, so that all valid discourse structures of a text can be determined. In the second part of the monograph, attention then shifts to alternative approaches to deriving valid discourse structures. The first approach relies primarily on discourse markers for shallow rhetorical parsing and employs, as a result of an in-depth corpus analysis, manually designed rules covering more than 450 English cue phrases such as because, however, and in addition, as well as punctuation marks. The second approach adds to plain discourse markers knowledge of surface-oriented lexical co-occurrence data, syntactic criteria (such as part-of-speech categories), and lexical similarity measures based on semantic relation information in order to identify text segments and their rhetorical organization. Given this knowledge-richer setting, discourse parsing rules were automatically derived by applying machine learning techniques (the C4.5 decision-tree algorithm) to data obtained from three corpora of manually annotated discourse trees. All these approaches are meticulously and lucidly described by providing various algorithm schemata for relevant computation steps. Empirical studies are then concerned with the role that discourse markers play in properly segmenting texts into elementary text units and in signaling rhetorical relations that hold between the text segments they connect. The correctness of the discourse trees built by the parser is judged intrinsically, by comparing automatically derived trees with ones that have been built manually, as well as extrinsically, by evaluating the impact automatically derived discourse trees have on properly solving natural language processing problems such as the summarization of texts. In the third part of the book, the utility of computing discourse structures is empirically assessed in the context of such a text summarization (i.e., extraction) task. The approach advocated by Marcu is readily applicable to this problem, since the representation structures it yields offer implicit content salience orderings in terms of the hierarchical tree structure and the distinction of important information contained in the nucleus and less-important information contained in the satellite portion of text spans, all of which are of immediate relevance for summarization purposes. The main hypothesis to be confirmed is whether or not discourse structures can be successfully exploited in a practical summarization setting. In a methodological experiment, evidence is gathered that text structures such as those mentioned above indeed effectively contribute to identifying the most important units of a text. A discoursestructure-based summarization algorithm that builds on these principles implements a simple salience metric that interprets the tree structure generated by the simple cue-phrase-based text-structure parser. A comparative evaluation reveals that this approach significantly outperforms two baseline algorithms (lead sentence and random sentence selection) and Microsoft\u2019s Office 97 summarizer. Considering the structure of discourse to be the paramount factor in determining salience, and incorporating a variety of additional position-, title-, text-tree-, and lexically-based summarization heuristics, a simple GSAT-style learning mechanism is presented that optimizes a linear combination of seven single salience metrics (in terms of combined recall and precision). This way, a significant increase in the performance of the discourse-based summarizer is achieved (yet parameter tuning is clearly dependent on the given text genre and compression rate!). Marcu\u2019s monograph presents a cornerstone in the computational treatment of texts. It has formal merits, as it provides a model-theoretic framework for the study of text coherence structures, in general, and the study of RST, in particular. It has computational merits, as it provides alternative ways of deriving text-structure descriptions automatically and inexpensively (i.e., avoiding full, in-depth text understanding) and distinguishes, given the a priori axiomatization, valid text structures from invalid ones. It has methodological merits, as it incorporates machine learning techniques for automatically acquiring the rules needed for discourse parsing and discourse-structuredriven summarization. Finally, it has empirical merits, as algorithms are tested and validated under different experimental conditions. Marcu also frankly admits that his work ignores the wealth of linguistic constructs that have been shown to be important in text understanding. Such phenomena include focus, topicality, cohesion and reference, pragmatics, and so on. Hence, the notion of validity being proposed is a constrained one, and it has to be weighed carefully against the notion of adequacy and expressiveness of the representation structures derivable therefrom. Still, the author claims that these phenomena can be couched in his formal framework as well. Additionally, one might mention the crucial role of domain-knowledge-dependent inferences and their interaction with building text structures in the absence of explicit cue phrases. Further open issues are the granularity of the text units that span rhetorical relations (e.g., the phrasal as opposed to the clausal or \u201cclause-like\u201d level) and the impact of the text genres under scrutiny. Finally, the dependence on basic assumptions and constructs underlying RST, despite the author\u2019s attempt to abstract away from it as much as possible, might be more prevalent than is acknowledged. The book spans a wide variety of issues in a well-structured, reader-friendly way, and it is easy to understand even in its technical passages. Hence, it can be highly recommended for graduate courses on text analysis. Students are given an outstanding example of the current research paradigm of computational linguistics, which includes formal, algorithmic, methodological, and empirical contributions. And they also may learn how scientific results can be communicated in a rigorous though comprehensible manner. Mann, William C. and Sandra A. Thompson. 1988. Rhetorical Structure Theory: Toward a functional theory of text organization. Text, 8(3):243\u2013281. Udo Hahn is a professor of computational linguistics at Albert-Ludwigs-Universit\u00a8at Freiburg, Germany. His methodological interests include text parsing, knowledge and discourse representation, and learning from texts. He has worked mainly on text analysis applications such as text summarization, knowledge extraction and text mining, and document retrieval. Hahn can be contacted via www.coling.uni-freiburg.de/\ue187hahn. This is a large volume, and it contains multitudes. Semantics is construed in a broad sense as the study of how meaning is communicated through the medium of language in a social context, taking account of inferences the hearer is expected to make on the basis of such factors as linguistic knowledge per se, context and \u201cco-text,\u201d encyclopedic knowledge, conventions of politeness and cooperative behavior, and the relative social status of speaker and hearer. The book ranges over a variety of approaches that have addressed these issues, including philosophy of language, lexicography, formal (logicbased) and cognitive semantics, frame-based knowledge representation, pragmatics, and anthropology. However, the result is more than a catalogue of theoretical tools and frameworks; throughout the book, Allan keeps in view an underlying philosophy that \u201cmeaning is cognitively and functionally motivated.\u201d Chapters 1 and 2 introduce fundamental notions such as sense and reference, extension and intension, compositionality, and speech acts. Chapters 3\u20135 deal with aspects of lexical semantics: Chapter 3 concerns the structure and content of lexical entries, Chapter 4 investigates the extent to which individual morphemes can be assigned semantic interpretations, and Chapter 5 contains an illuminating discussion of aspects of nonliteral word meaning such as connotation, euphemism, dysphemism, and jargon. Chapters 6 and 7 introduce the formal apparatus of propositional and predicate logic and the lambda calculus, and discuss notions of consequence such as semantic entailment and conversational and conventional implicatures. Chapters 8\u201310 are concerned with \u201ccognitive and functional approaches to semantics,\u201d that is, approaches whose theoretical constructs are claimed to have some form of \u201cpsychological reality\u201d or are motivated in terms of their \u201ccommunicative functions.\u201d These chapters review topics such as frames and scripts, componential analysis, classifiers, color categories across languages, prototypes, and stereotypes. Chapters 11\u201313 address various issues in clausal and nominal semantics. Chapter 11 concerns modality, tense, and thematic roles, whereas Chapter 12 discusses different approaches to the semantics of verbs and other predicates. Chapter 13 grapples with some of the intricacies of quantification, number, and countability in English noun phrases, using generalized quantifier theory and a variant of ensemble theory. As far as I know, this is the first published tutorial account of the latter, which has previously only been accessible to students in Harry Bunt\u2019s rather challenging monograph (1985). We are told that Chapters 11\u201313 \u201cdemonstrate the application of formal methods of semantic analysis to a corpus of data.\u201d This section is likely to disappoint computational linguists, who will understand the term corpus in a different way, since the data in these chapters consist of a series of singlesentence (and mostly single-clause) examples apparently constructed by the author. The author is clearly in sympathy with the cognitive semantics school, which claims to uncover \u201cpsychologically real\u201d structures and processes involved in language use. It\u2019s not always clear to me from Allan\u2019s account what the various claims for psychological reality amount to (such as that for \u201clinguistic categories, semantic fields, frames and the like,\u201d page 288). The methodology displayed tends to follow the standard practice of linguistics textbooks in postulating abstract analyses of examples constructed by the analyst, the reader being invited to share the analyst\u2019s intuitions about their acceptability and interpretation; there is little appeal to experimental or neurological evidence, for example. There are some fascinating discussions of various senses of words such as back (pages 289ff.) and over (pages 330\u2013331), extended from their basic senses that are presumed to be rooted in direct physical perception. However, these do not give rise to productive procedures that could reliably generate extended senses for equivalent words in other languages, for instance, or other words denoting physical relations or body parts. An attempt to map out some common ground between the cognitive and formal approaches is far from convincing (pages 288\u2013289, emphasis added): (First premise) Formal representations are created by human minds and are interpretable by human minds. Therefore, they have cognitive reality ... (Second premise) The informal metalanguages of the cognitivists ... are creations of deliberate, consciously contrived artifice, just as much as any formal metalanguages are. (Conclusion) Formalists, cognitivists and functionalists all use contrived metalanguages that have cognitive reality. As an introduction to formal semantics, this book does not supersede established classics such as Gamut (1991). Definitions are sometimes unsatisfactory, effectively substituting one imprecise term for another, as when Grice\u2019s (1975) conventional implicature (CI) is defined as \u201cimplies ... but does not entail\u201d (page 189). The examples of CI that are offered include these: all gold implies that the ensemble of gold is nonempty (page 437) and four eggs implies at least two eggs (page 189). But these are surely different phenomena, the former being a defeasible convention and the latter an arithmetical consequence of the meanings of four and two. Likewise, in the chapter on quantifiers, few students is glossed as \u201cvery-much-less than\u201d all contextually relevant students (page 433), though how much less counts as \u201cvery-much-less than\u201d itself depends on context and assumptions about prior expectations (Moxey and Sanford 1993). Every computational linguist should own at least one semantics textbook. Allan\u2019s book stands apart from many other texts in the way it conveys a real sense of the variety and fecundity of language as spoken by living, breathing human beings, rather than as a source of intriguing logicophilosophical puzzles. Nonspecialists will certainly find it an informative, albeit uneven, conspectus of paradigms and areas of inquiry in linguistic semantics, with the bonus that it is actually fun to read. The study of intonation is an expanding field, extending beyond core linguistic disciplines such as syntax, semantics, and pragmatics into areas as wide-ranging as psycholinguistics, neurolinguistics, discourse analysis, and emotion research. Intonation is also currently the prime focus of attention in speech synthesis research and is rapidly gaining ground in speech recognition. This expansion has been reflected in a number of workshops and conferences devoted solely to intonation and its interfaces, one of which was the ESCA Workshop on Intonation in Athens in September 1997. The volume reviewed here is one of two collections of papers based on contributions to this workshop. The other collection has appeared as a special issue of Speech Communication (33(4), March 2001). The book consists of four main sections\u2014Prominence and Focus, Boundaries and Discourse, Intonation Modelling, and Intonation Technology\u2014along with an introduction (Antonis Botinis) and a historical overview (Mario Rossi). In this review, I shall concentrate on papers that relate intonation to semantic, pragmatic, or discourse functions and leave the papers dealing solely with speech or phonetics for a review in a journal specializing in those areas. In the Prominence and Focus section, Julia Hirschberg and Cinzia Avesani\u2019s paper, \u201cProsodic Disambiguation in English and Italian,\u201d investigates to what degree speakers of English and Italian use intonational means to disambiguate semantically and syntactically ambiguous sentences. The authors found that, with the exception of quantifier scope, semantic ambiguities were generally more clearly disambiguated than syntactic ones. This was true for both languages. Regarding the semantic ambiguities, the two languages had similar strategies: the scope of negation was disambiguated by phrasing, and the differences in scope of focus-sensitive operators were distinguished by means of pitch accent placement. The authors note that although speakers were aware of the distinctions, they often produced a neutral rendition that would be felicitous for either interpretation, presumably because they read the tokens within a context that already resolved the ambiguity. We learn from this that although context is necessary for eliciting the correct reading, it may at the same time dispense with the reason to disambiguate. In the Boundaries and Discourse section, all three papers have something to offer for readers of Computational Linguistics. Vincent van Heuven and Judith Haan\u2019s \u201cPhonetic Correlates of Statement versus Question Intonation in Dutch\u201d is based on both production and perception experiments. The authors show that although questions as a category have a number of acoustic properties that clearly distinguish them from statements, each question type has a distinct profile of its own in terms of F0 (fundamental frequency, the perceptual correlate of which is pitch). These question types are wh-questions (lexical and syntactic marking), yes-no questions (which in Dutch are marked syntactically), and declarative questions (syntactically indistinguishable from statements). One major cue for the perception of questions is a sentence-final rise in pitch that was never found in statements. The smaller the number of lexicosyntactic indicators as to interrogativity, the higher the rise in pitch and the greater the incidence of such a rise. Other cues include the pitch range, height, and overall shape (e.g., downward or upward trends) across the whole sentence. Monique van Donzel and Florien Koopmans-van Beinum\u2019s \u201cPitch Movements and Information Structure in Spontaneous Dutch Discourse\u201d confirms previous findings that new information is more often accented than inferrable information. The authors establish the following hierarchy of accentability: new information > inferrable information > verbs > modifiers > discourse markers and evoked information. Discourse boundaries, assigned on the basis of a discourse model developed by the authors in earlier work, are realized with rising pitch (labeled as nonfinal by naive listeners) more often than previously reported. The authors also show that pitch height depends on neither newness nor the type of discourse boundary. Speakers varied a great deal with regard to how often they marked discourse boundaries with pitch variation as opposed to, say, pausing. However, despite these realizational differences, naive listeners perceived prominences and boundaries to a comparable extent across speakers, indicating that they are flexible enough to adapt their perceptual criteria to the current speaker. Anne Wichman, Jill House, and Toni Rietveld\u2019s \u201cDiscourse Constraints on F0 Peak Timing in English\u201d is a double study of Southern Standard British English, using natural uncontrolled data and a carefully designed corpus of read paragraphs. The timing of F0 peaks is shown to be dependent on where the accent falls within a discourse unit: peaks were later in paragraph-initial position (equivalent in this study to discourse-topic-initial position) than in paragraph-internal position, and sentenceinitial accents were in turn later than sentence-final ones. The authors conclude that topic-initiality exerts a strong rightward push on F0 peaks, even causing them to occur outside the accented syllable. Predictably, discourse structure is also found to affect F0 peak height, topic-initiality leading to higher peaks than topic-mediality. In the Intonation and Technology section, G\u00a8osta Bruce, Marcus Filipson, Johan Frid, Bj\u00a8orn Granstr\u00a8om, Kjell Gustafson, Merle Horne, and David House\u2019s \u201cModelling of Swedish Text and Discourse Intonation in a Speech Synthesis Framework\u201d provides an overview of an intonation model that was originally based on single-utterance laboratory speech. The model has now been extended to cover dialogues and multispeaker conversations, incorporating information on lexical semantics and discourse and textual structure. An important step in the research program is model-based resynthesis, whereby a synthetic F0 contour is superimposed on the original utterance. The F0 values for resynthesis are calculated on the basis of the symbolic utterance-level representation (pitch accents and boundary tones only) of the original. The differences between calculated and original F0 values, such as overall trends and shifts up and down in F0 range and height, are related to the analysis of the text in order to extract parameter values that can be fed into the text-to-speech implementation. It is clear that this volume is aimed at readers who already have a basic knowledge of intonation and know what they are looking for. Since most of the chapters deal with highly specialized topics, each one is likely to be read in isolation. However, an incentive to read more could have been provided, had each of the four sections been accompanied by a synopsis of its main themes and common threads. Martine Grice is an assistant professor at Saarland University, Germany. Her main research interest is intonation theory, in particular, the structure of tonal representations. She has also developed schemes for the database annotation of tonal and junctural phenomena, both for Standard German (GToBI) and for a number of varieties of Italian (IToBI). Grice\u2019s address is Institute of Phonetics, FR.4.7, Saarland University, P.O. Box 151150, D-66041 Saarbr\u00a8ucken, Germany; e-mail: mgrice@coli.uni-sb.de. (IBM T. J. Watson Research Center and Educational Testing Services) New York: Oxford University Press, 2000, xi+227 pp; hardbound, ISBN 0-19-823842-8, $74.00, \u00a345.00; paperbound, ISBN 0-19-925086-3, $21.95, \u00a314.99 As the editors of this volume remind us, polysemy has been a vexing issue for the understanding of language since antiquity.1 For half a century, it has been a major bottleneck for natural language processing. It contributed to the failure of early machine translation research (remember Bar-Hillel\u2019s famous pen and box example) and is still plaguing most natural language processing and information retrieval applications. A recent issue of this journal described the state of the art in automatic sense disambiguation (Ide and V\u00b4eronis 1998), and Senseval system competitions have revealed the immense difficulty of the task (http://www.sle.sharp.co.uk/senseval2). However, no significant progress can be made on the computational aspects of polysemy without serious advances in theoretical issues. At the same time, theoretical work can be fostered by computational results and problems, and language-processing applications can provide a unique test bed for theories. It was therefore an excellent idea to gather both theoretical and applied contributions in the same book. Yael Ravin and Claudia Leacock are well-known names to those who work on the theoretical and computational aspects of word meaning. In this volume, they bring together a collection of essays from leading researchers in the field. As far as I can tell, these essays are not reprints or expanded versions of conference papers, as is often the case for edited works; instead, they seem to have been specially commissioned for the purposes of this book, which makes it even more exciting to examine. The book is composed of 11 chapters. It is not formally divided into parts, but chapters dealing more specifically with the computational aspects of polysemy are grouped together at the end (and constitute about one-third of the volume). Chapter 1 is an overview written by the volume editors. Yael Ravin and Claudia Leacock survey the main theories of meaning and their treatment of polysemy. These include the classical Aristotelian approach revived by Katz and Fodor (1963); Rosch\u2019s (1977) prototypical approach, which has its roots in Wittgenstein\u2019s Philosophical Investigations (1953); and the relational approach recently exemplified by WordNet (Fellbaum 1998), which (although the authors do not mention it) can be traced back to Peirce\u2019s (1931\u20131958) and Selz\u2019s (1913, 1922) graphs and which gained popularity with Quillian\u2019s (1968) semantic networks. In the course of this overview, Ravin and Leacock put the individual chapters into perspective by relating them to the various theories. In Chapter 2, \u201cAspects of the Micro-Structure of Word Meanings,\u201d D. Alan Cruse addresses the issue of the extreme context-sensitivity of word meaning, which can result in an almost infinite subdivision of senses. However, Cruse believes that there are \u201cregions of higher semantic density\u201d within this extreme variability, which he calls sense-nodules, \u201clumps of meaning with greater or lesser stability under contextual change.\u201d As Cruse admits, this is only a metaphor, and as such, may not be highly useful to the researcher. In the rest of the chapter, Cruse attempts to build a typology of these nodules, listing their properties and proegions of higher semantic density\u201d within this extreme variability, which he calls sense-nodules, \u201clumps of meaning with greater or lesser stability under contextual change.\u201d As Cruse admits, this is only a metaphor, and as such, may not be highly useful to the researcher. In the rest of the chapter, Cruse attempts to build a typology of these nodules, listing their properties and providing tests to detect them. The tests (e.g., the zeugma effect in sentences such as John and his driving license expired yesterday) are not entirely new (e.g., Quine 1960; Cruse 1986; Geeraerts 1993), but are integrated here into a coherent framework that places context-dependency at the very heart of the theory. Chapter 3 by Christiane Fellbaum is devoted to \u201cautotroponymy.\u201d This term requires a two-step explanation. Troponyms are verb hyponyms, referring to specific manners of performing actions denoted by other verbs. For example, in English, stammer, babble, whisper, and shout are troponyms of talking. Autotroponymy is a special case that occurs when the verbs linked by this relation share the same form, as in The children behaved /The children behaved well. The author explains autotroponymy in terms of conflation of a meaning component not expressed on the surface. For example, in The children behaved, the verb includes a hidden adverbial (well /satisfactorily/appropriately). Fellbaum gives a typology of autotroponyms that is based on the nature of the conflated element (noun, adjective, adverbial), and she discusses their syntactic and semantic properties in detail. In Chapter 4, \u201cLexical Shadowing and Argument Closure,\u201d James Pustejovsky explores verbs such as butter, which block the expression of a generic argument, as in *Mary buttered her bread with butter, while allowing for a specific one, as in Mary buttered her bread with expensive butter from Wisconsin (see Levin 1993), and verbs such as risk, which can occur in contradictory contexts with roughly the same meaning, as in Mary risked death to save her son /Mary risked her life to save her son (see Fillmore and Atkins 1995). Pustejovsky introduces the concept of \u201clexical shadowing,\u201d which he defines as \u201cthe relation between an argument and the underlying semantic expression, which blocks its syntactic projection in the syntax.\u201d For example, the underlying semantics of the verb butter \u201cshadows\u201d the expression of the substance that is spread and allows only for specialization of the shadowed argument. For verbs such as risk, the shadowing is of a different type: it is the expression of one argument that shadows the expression of another, in a strictly complementary fashion. Pustejovsky explains these cases of argument optionality or complementarity in the framework of the Generative Lexicon (Pustejovsky 1995) and its various devices, among which \u201ccoercion\u201d plays a central role. Chapter 5, by Charles Fillmore and Sue Atkins, is a case study in lexicography. They analyze the sense divisions and definitions of the verb crawl in various dictionaries and compare them with corpus evidence from the British National Corpus. It is well known that dictionaries exhibit large discrepancies, and although they claim to be based on the analysis of corpus data, many sense distinctions that show up in a corpus are not reflected in dictionary entries. This is not entirely unexpected, since after all, no dictionary claims exhaustive coverage of a language, and some selection must be made by the lexicographer. This is even an explicit goal in four of the six dictionaries examined here, which are learners\u2019 dictionaries that attempt to illustrate the \u201ccore\u201d uses of words for learners of English. It is striking, however, to see the extent to which lexicographers differ regarding their choices and assessment of what constitutes an important meaning a learner should acquire. Fillmore and Atkins are perfectly right in noting that lexicographers lack objective criteria for sense division and information extraction from corpora. The FrameNet project they describe in an appendix (see http://www.icsi.berkeley.edu/\ue187framenet/) is an attempt to achieve a systematic understanding and description of the meanings of lexical items and grammatical constructions by looking at a large number of attested examples, sorting them according to the conceptual structures (semantic \u201cframes\u201d) that underlie their meanings, and describing the associated information in terms of semantic roles, phrase types, and grammatical functions. The numerous observations regarding sense connections in the corpus examples result in a network-like organization of meanings, which can be used in both monolingual and bilingual lexicography. The last section of the chapter illustrates this possibility using the verb ramper, the French equivalent of to crawl. Chapter 6, \u201c\u2018The Garden Swarms with Bees\u2019 and the Fallacy of \u2018Argument Alternation\u2019\u201d by David Dowty, comes back to the argument problem already tackled by Fellbaum and Pustejovsky in their respective chapters and proposes syntactic structures as an explanatory principle for alternations in meaning. The author is concerned with agent / location alternations such as Bees swarm in the garden / The garden swarms with bees. He departs from the usual point of view that such pairs express the same meaning and differ only in syntactic form. Using the large set of examples in Salkoff (1983), Dowty groups verbs that participate in such alternations into five semantic classes and then shows that the two forms exhibit many semantic differences related to the informational structure of the sentence. The locative-subject form makes the location the topic of discourse, with the predicate ascribing an abstract property to the location. Some tests show the difference in meaning. For example, the with-phrase object must be semantically \u201cunquantified\u201d in the locative-subject form (compare A roach crawled on the wall / *The wall crawled with a roach), the locative-subject form is more suited to metaphor than the agent-subject form, and so forth. Chapter 7 by Cliff Goddard outlines Wierzbicka\u2019s \u201cnatural semantic metalanguage\u201d (NSM) approach to semantic analysis (Wierzbicka 1996, etc.), which is based on the idea that every language possesses a core of undefinable words (\u201csemantic primes\u201d). Complex expressions (words or grammatical constructions) can be described by means of explanatory reductive paraphrases composed of combinations of semantic primes. This \u201cdefinitional\u201d framework provides a diagnosis technique for detecting polysemy. For any given word, one can first assume that it has a single meaning and try to state it in a reductive paraphrase. If this turns out to be impossible and several paraphrases are needed to describe the word\u2019s range of uses, then the word has distinct meanings. For example, there is no single paraphrase in terms of primes that could predict the range of uses of the French word fille, meaning both daughter and girl, and therefore the word must be split into two distinct meanings. Using this test, Goddard shows that dictionaries very often posit false and unnecessary polysemy, and occasionally false monosemy. He also shows how the technique can be used on grammatical constructions, and he applies it in detail to have a VP expressions (have a stroll, have a chat, etc.). The chapter ends with a discussion of how aspects of figurative language can be handled within this framework. In Chapter 8, \u201cLexical Representations for Sentence Processing,\u201d George Miller and Claudia Leacock raise the following question: \u201cWhy isn\u2019t a dictionary a good theory of the lexical component of language?\u201d They share Fillmore and Atkins\u2019s dissatisfaction about dictionary making. For them, the main shortcoming of dictionaries is their lack of contextual information that would enable a user to make the correct association between senses and actual contexts. In their introduction, they give a convincing example from previous experiments. Schoolchildren given dictionary definitions of English words produced sentences such as Our family erodes a lot, which sounds bizarre until you read the definition of erode: \u2018eat out, eat away\u2019. According to Miller and Leacock, what is missing from dictionaries is a satisfactory treatment of the lexical aspects of sentence processing. The rest of the chapter is devoted to a discussion of the two types of context that can be used to associate a given context with a particular word sense: local context (the immediate neighbors of the word under focus) and topical context (the general topic or domain of the text or conversation). The authors show that local context cues are very precise when they occur, but often simply do not occur. On the other hand, topical context is very efficient in helping discriminate between homographs, but not very helpful for identifying the different senses of a polysemous word. Miller and Leacock consider the combination of the two sources to be a major avenue of research. Mark Stevenson and Yorick Wilks tackle this issue in Chapter 9, \u201cLarge Vocabulary Word Sense Disambiguation,\u201d in which they propose a methodology for combining several knowledge sources into a word sense disambiguation system. Their first source of information is syntactic in nature and is provided by the Brill partof-speech tagger. The semantic information present in the local context is then used in two ways. The overlap between Longman Dictionary of Contemporary English definitions and the local context is computed by means of an improved version of Cowie, Guthrie, and Guthrie\u2019s (1992) simulated-annealing technique, and selectional restrictions are resolved by means of LDOCE semantic classes. The larger context is handled with techniques that map it to the subject categories provided by LDOCE for each sense (\u201cpragmatic codes\u201d). The efficiency of each of these modules taken separately ranges from 44 to 79 percent, but Stevenson and Wilks show that using machine learning techniques, the modules can be combined in an efficient way to produce 90 percent correct disambiguation, which is quite high for an unrestricted vocabulary system. In Chapter 10, \u201cPolysemy in a Broad-Coverage Natural Language Processing System,\u201d William Dolan, Lucy Vanderwende, and Steven Richardson describe the approach to polysemy processing taken in the MS-NLP broad-coverage natural language understanding system. The core of their system is MindNet, a network-structured computational lexicon extracted from machine-readable dictionaries (MRD) augmented with corpus information. MindNet uses the same general approach as the MRD-based spreading activation networks proposed by V\u00b4eronis and Ide (1990), although in a much more sophisticated version including labeled connections, backward links, weighted paths, and so on. The authors depart from most computational approaches to polysemy in that they believe that word meaning is \u201cinherently flexible,\u201d that making predefined inventories of discrete senses is unsuitable for broad-coverage applications, and that no sharp boundaries should be drawn between senses. Their approach is reminiscent of Cruse\u2019s, presented earlier in this book. For these authors, \u201cunderstanding\u201d is no more than identifying an activation pattern in the network. In previous publications, Hinrich Sch\u00a8utze held a position similar to Dolan, Vandervende, and Richardson\u2019s with respect to predefined sense inventories. For Sch\u00a8utze, many problems require discrimination among senses but do not require explicit sense labeling, and the techniques he has proposed extract the sense divisions from the corpus itself (see Sch\u00a8utze 1998): a sense is a group of contextually similar occurrences of a word. This approach is almost the opposite of Goddard\u2019s. In Chapter 11, Sch\u00a8utze looks at word sense disambiguation from the perspective of connectionism. After surveying some of the literature on disambiguation, he presents an algorithm that has grown out of two major concerns in connectionist research: psychological plausibility and large-scale applicability. He describes an application to information retrieval that demonstrates that his algorithm can be applied to very large text collections (500 megabytes of text from the Wall Street Journal). The most noticeable feature of this book is probably its wide range of contributors and the broad scope of the topics it encompasses. As the title implies, it addresses both theoretical and computational aspects of polysemy, and within these two areas, very different research trends are pursued. The book gives a very good overall picture of current issues in polysemy and of the diverse ways of approaching the topic. It should therefore hold an important place on the shelves of any researcher in the fields of lexical semantics and word sense disambiguation, and will certainly be valued by many of our graduate students. The wide-angle snapshot offered by this book also reveals a very striking fact about current lexical semantics. Apart from one chapter, all theoretical discussions are supported solely by invented examples. Lexical semantics, and probably semantics in general, has not yet made the paradigm shift that has occurred or is occurring in other branches of linguistics, such as syntax, where empirical evidence now replaces intuition as the normal body of data to be studied. Another recent book (Sampson 2001) quite brilliantly shows how the lack of objective evidence has been misleading linguistic research for decades and has placed the discipline on the fringe of modern science. The lack of objective evidence is probably even more dangerous in semantics than in other areas of linguistics. The extreme flimsiness of introspection-based tests is acknowledged by lexical semanticists themselves\u2014for instance, how much agreement would there be on whether or not a given coordination is a zeugma?\u2014and such tests make it almost impossible for semantics to satisfy the minimal requirement that science has demanded since Karl Popper, that of refutability. Interestingly enough, the one chapter that does use corpus examples (Chapter 5 by Fillmore and Atkins) pertains to lexicography. Lexicographers indeed have a long tradition of examining objective evidence, which computer tools and electronic corpora have made it possible to systematize. However, several chapters (Chapter 5 by Fillmore and Atkins, Chapter 7 by Goddard, Chapter 8 by Miller and Leacock) express their dissatisfaction with current dictionaries, on the grounds that they lack theoretical criteria to back their organization. It is also worth noting that the only computational approaches to word sense disambiguation able to claim some minimal degree of efficiency are linguistically blind ones (like those reported in this book), as if an insurmountable gap existed between theories and applications. A paradigm shift in lexical semantics is therefore not just a scientific necessity; it is also a practical one. I am convinced that no major breakthrough in language-processing applications and lexicography can be made until theories of meaning are based on the observation of real data."
    },
    "J02-1007": {
        "title": "Briefly Noted @authors: Briefly Noted, Graeme R. Newman @citationVelocity: 6 @fieldsOfStudy: None @influentialCitationCount: 5 @isOpenAccess: False @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 151 @numCiting: 0 @s2FieldsOfStudy: History @topics:  @venue:  @year: 2010",
        "abstract": "the 130,000-word SUSANNE Corpus. This corpus has had several lives. It began as a subset of the Brown Corpus, selected from four text categories: press reportage, belles lettres, \u201clearned\u201d prose, and adventure fiction. That subcorpus was manually analyzed by Alvar Elleg\u02daard and colleagues at Gothenburg University and came to be known as the Gothenburg Corpus. Sampson\u2019s team then analyzed this same subcorpus in greater detail, together with a small sample of speech from the London-Lund Corpus, resulting in the SUSANNE Corpus. The book under review documents the hundreds of decisions required for this annotation process. This resource should prove useful for researchers faced with these same decisions during corpus analysis. However, the level of detail will sometimes overwhelm computational and corpus linguists attempting to develop automatic taggers and parsers. Many of these decisions clearly require a human analyst, and thus it is not clear how useful they will be for machine processing. Others end up seeming arbitrary, despite the detailed justifications. For example, Flagstaff\u2014my home town\u2014is tagged as a common noun (page 87), because the word originally referred to a flagpole. In contrast, Greenwood is tagged as a proper noun (page 88), because it was originally the surname of a colonist. These principles seem clear, even if nearly impossible to implement automatically. However, I had trouble reconciling those decisions with the tagging of Red as a proper noun (masculine forename) in Red Hogan but as an adjective in V. E. (Red) Berry (page 138). I suspect that Sampson would not be bothered by such criticisms. He did not set out to develop the perfect annotation scheme, and he readily agrees that his scheme\u2014like all others\u2014has controversial points. What makes this scheme unique is its attempt to be comprehensive and explicit about all details. In this respect, corpus researchers will find themselves relying on this reference book increasingly as they work toward the goal of readily exchangeable resources.\u2014Douglas Biber, Northern Arizona University Briefly Noted"
    },
    "J02-1004": {
        "title": "Syllable-Pattern-Based Unknown-\nMorpheme Segmentation and Estimation\nfor Hybrid Part-of-Speech Tagging of\nKorean @authors: G. G. Lee, Jeongwon Cha, Jong-Hyeok Lee @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 1 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 41 @numCiting: 16 @s2FieldsOfStudy: Computer Science, Linguistics @topics: Syllable, Part-of-speech tagging, Trigram, Text corpus, Brown Corpus, Logic programming, Experiment, Binary code, Dictionary @venue: International Conference on Computational Logic @year: 2002",
        "abstract": "Part-of-speech (POS) tagging involves many difficult problems, such as insufficient amounts of training data, inherent POS ambiguities, and (most seriously) many types of unknown words. Unknown words are ubiquitous in any application and cause major tagging failures in many cases. Since Korean is an agglutinative language, it presents more serious problems with unknown morphemes than with unknown words because more than one morpheme can be unknown in a single word and morpheme segmentation is usually very difficult. Previous techniques for guessing unknown words mostly utilize the guessing rules to analyze the word features by looking at leading and trailing characters. Most of them employ the analysis of trailing characters and other features such as capitalization and hyphenation (Kupiec 1992; Weischedel et al. 1993). Some of them use more morphologically oriented word features such as suffixes, prefixes, and character lengths (Brill 1995; Voutilainen 1995). The guessing rules are usually handcrafted using knowledge of morphology but sometimes are acquired automatically using lexicons and corpora (Brill 1995; Mikheev 1996; Oflazer and T\u00a8ur 1996). Previously developed methods for guessing unknown morphemes in Korean are not much different from the methods used for English. Basically, they rely on the rules that reflect knowledge of Korean morphology and word formation. The usual way of handling unknown morphemes is to guess all the possible POS tags for an unknown morpheme by checking connectable functional morphemes in the same eojeol (Kang 1993).2 However, in this way, it is only possible to guess probable POS tags for a single unknown morpheme when it occurs at the beginning of an eojeol. Unlike in English, in Korean, more than one unknown morpheme can appear in a single eojeol because an eojeol can include complex components such as Chinese characters, Japanese words, and other foreign words. If an eojeol contains more than one unknown morpheme or if the unknown morphemes appear in other than first position in the eojeol, all previous methods fail to efficiently estimate them. This is the reason why we try to avoid conventional guessing rules using word morphology features such as those proposed in Mikheev (1996) and Oflazer and T\u00a8ur (1996).3 In this paper, we propose a syllable-pattern-based generalized unknown-morpheme estimation method using a morpheme pattern dictionary that enables us to treat unknown morphemes in the same way as registered known morphemes, and thereby to guess them regardless of their numbers or positions in an eojeol. The method for estimating unknown morphemes using the morpheme pattern dictionary in Korean needs to be tightly integrated into morphological analysis and POS disambiguation systems. POS disambiguation has usually been performed by statistical approaches, mainly using the hidden Markov model (HMM) in English research communities (Cutting et al. 1992; Kupiec 1992; Weischedel et al. 1993). These approaches are also dominant for Korean, with slight improvements to accommodate the agglutinative nature of Korean. For Korean, early HMM tagging was based on eojeols. The eojeol-based tagging model calculates lexical and transition probabilities with eojeols as a unit; it suffers from severe data sparseness problems since a single eojeol consists of many different morphemes (Lee, Choi, and Kim 1993). Later, morpheme-based HMM tagging was tried; such models assign a single tag to a morpheme regardless of the space in a sentence. Morpheme-based tagging can reduce data sparseness problems but incurs multiple observation sequences in Viterbi decoding since an eojeol can be segmented in many different ways. Researchers then tried many ways of reducing computation due to multiple observation sequences, such as shared word sequences and virtual words (Kim, Lim, and Seo 1995) and two-ply HMM for morpheme unit computation but restricted within an eojeol (Kim, Im, and Im 1996). However, since statistical approaches take neighboring tags into account only within a limited window (usually two or three), sometimes the decision fails to cover important linguistic contexts necessary for POS disambiguation. Also, approaches using only statistical methods are inappropriate for idiomatic expressions, for which lexical terms need to be directly referenced. And especially, statistical approaches alone do not suffice for agglutinative languages, which usually have complex morphological structures. In agglutinative languages, a word usually consists of one or more stem morphemes plus a series of functional morphemes; therefore, each morpheme should receive a POS tag appropriate to its functional role to cope with the complex morphological phenomena in such languages. Recently, rule-based approaches, which learn symbolic tagging rules automatically from a corpus, have been reconsidered, to overcome the limitations of statistical approaches (Brill 1995). Some systems even perform POS tagging as part of a syntactic analysis process (Voutilainen 1995). Following the success of transformation-based approaches, attempts have been made to use transformation rules in systems for tagging Korean (Im, Kim, and Im 1996). However, in general, rule-based approaches alone are not very robust and are not portable enough to be adjusted to new tagsets or new languages. Also, they usually perform no better than their statistical counterparts (Brill 1995). To gain portability and robustness and also to overcome the limited coverage of statistical approaches, we need to somehow combine the two approaches to gain the advantages of each. In this paper, we propose a hybrid method that combines statistical and rule-based approaches to POS disambiguation and can be tightly coupled with generalized unknown-morpheme-guessing techniques."
    },
    "J02-1005": {
        "title": "Squibs and Discussions\nThe DOP Estimation Method Is Biased and\nInconsistent @authors: Mark Johnson @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 6 @isOpenAccess: False @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 73 @numCiting: 7 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Statistical parsing, Data-oriented parsing, Numerical analysis, Converge @venue: International Conference on Computational Logic @year: 2002",
        "abstract": "The data-oriented parsing or DOP approach to statistical natural language analysis has attracted considerable attention recently and has been used to produce statistical language models based on various kinds of linguistic representation, as described in Bod (1998). These models are based on the intuition that statistical generalizations about natural languages should be stated in terms of \u201cchunks\u201d or \u201cfragments\u201d of linguistic representations. Linguistic representations are produced by combining these fragments, but unlike in stochastic models such as Probabilistic Context-Free Grammars, a single linguistic representation may be generated by several different combinations of fragments. These fragments may be large, permitting DOP models to describe nonlocal dependencies. Usually the fragments used in a DOP model are themselves obtained from a training corpus of linguistic representations. For example, in DOP1 or Tree-DOP the fragments are typically all the connected multinode trees that appear as subgraphs of any tree in the training corpus. This note shows that the estimation procedure standardly used to set the parameters or fragment weights of a DOP model (see, for example, Bod [1998]) is biased and inconsistent. This means that as sample size increases, the corresponding sequence of probability distributions estimated by this procedure does not converge to the true distribution that generated the training data. Consistency is usually regarded as the minimal requirement any estimation method must satisfy (Breiman 1973; Shao 1999), and the inconsistency of the standard DOP estimation method suggests it may be worth looking for other estimation methods. Note that while the bulk of DOP research uses the estimation procedure studied here, recently there has been research that has used other estimators for DOP models (Bonnema, Buying, and Scha 1999; Bod 2000), and it would be interesting to investigate the statistical properties of these estimators as well. Depictions of three different derivations of the same tree representation of Alex likes pizza, with arrows indicating the sites of tree fragment substitutions."
    },
    "W15-4007": {
        "title": "Observed versus latent features for knowledge base and text inference @authors: Kristina Toutanova, Danqi Chen @citationVelocity: 143 @fieldsOfStudy: Computer Science @influentialCitationCount: 209 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 699 @numCiting: 22 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Knowledge base, Feature model, Kilobyte, Archive, Test case, Benchmark (computing) @venue: Workshop on Continuous Vector Space Models and their Compositionality @year: 2015",
        "abstract": "In this paper we show the surprising effectiveness of a simple observed features model in comparison to latent feature models on two benchmark knowledge base completion datasets, FB15K and WN18. We also compare latent and observed feature models on a more challenging dataset derived from FB15K, and additionally coupled with textual mentions from a web-scale corpus. We show that the observed features model is most effective at capturing the information present for entity pairs with textual relations, and a combination of the two combines the strengths of both model types."
    },
    "W10-4158": {
        "title": "Jumping Distance based Chinese Person Name Disambiguation1 @authors: Yu Hong, F. Pei, Yuehui Yang, Jianmin Yao, Qiaoming Zhu @citationVelocity: 0 @fieldsOfStudy: Computer Science, Mathematics @influentialCitationCount: 0 @isOpenAccess: False @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 0 @numCiting: 15 @s2FieldsOfStudy: Computer Science, Mathematics, Computer Science @topics: Word-sense disambiguation, N-gram, Grams, String (computer science), Cluster analysis, Algorithm @venue: CIPS-SIGHAN @year: 2010",
        "abstract": "In this paper, we describe a Chinese person name disambiguation system for news articles and report the results obtained on the data set of the CLP 2010 Bakeoff-31. The main task of the Bakeoff is to identify different persons from the news stories that contain the same person-name string. Compared to the traditional methods, two additional features are used in our system: basis, we propose a two-stage clustering algorithm to improve the low recall."
    },
    "S13-2009": {
        "title": "ManTIME: Temporal expression identification and normalization in the\nTempEval-3 challenge @authors: Michele Filannino, Gavin Brown, G. Nenadic @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 3 @isOpenAccess: False @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 30 @numCiting: 12 @s2FieldsOfStudy: Computer Science, Computer Science @topics: P (complexity), WordNet, Shallow parsing, Conditional random field, Video post-processing, Open-source software, Logic programming, Test data @venue: International Workshop on Semantic Evaluation @year: 2013",
        "abstract": "This paper describes a temporal expression identification and normalization system, ManTIME, developed for the TempEval-3 challenge. The identification phase combines the use of conditional random fields along with a post-processing identification pipeline, whereas the normalization phase is carried out using NorMA, an open-source rule-based temporal normalizer. We investigate the performance variation with respect to different feature types. Specifically, we show that the use of WordNet-based features in the identification task negatively affects the overall performance, and that there is no statistically significant difference in using gazetteers, shallow parsing and propositional noun phrases labels on top of the morphological features. On the test data, the best run achieved 0.95 (P), 0.85 (R) and 0.90 (F1) in the identification phase. Normalization accuracies are 0.84 (type attribute) and 0.77 (value attribute). Surprisingly, the use of the silver data (alone or in addition to the gold annotated ones) does not improve the performance."
    },
    "W11-1303": {
        "title": "Two Multivariate Generalizations of Pointwise Mutual Information @authors: Tim Van de Cruys @citationVelocity: 0 @fieldsOfStudy: None @influentialCitationCount: 2 @isOpenAccess: False @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 51 @numCiting: 16 @s2FieldsOfStudy: Computer Science @topics:  @venue: Proceedings of the Workshop on Distributional Semantics and Compositionality @year: 2011",
        "abstract": "Since its introduction into the NLP community, pointwise mutual information has proven to be a useful association measure in numerous natural language processing applications such as collocation extraction and word space models. In its original form, it is restricted to the analysis of two-way co-occurrences. NLP problems, however, need not be restricted to twoway co-occurrences; often, a particular problem can be more naturally tackled when formulated as a multi-way problem. In this paper, we explore two multivariate generalizations of pointwise mutual information, and explore their usefulness and nature in the extraction of subject verb object triples."
    },
    "P12-2013": {
        "title": "Genre Independent Subgroup Detection in Online Discussion Threads: A\nPilot Study of Implicit Attitude using Latent Textual Semantics @authors: Pradeep Dasigi, Weiwei Guo, Mona T. Diab @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 1 @isOpenAccess: False @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 12 @numCiting: 15 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Unsupervised learning, Conversation threading, Wikipedia, Topic model, Social media, Named entity, Sensor, Tag (metadata), Complementarity (physics) @venue: Annual Meeting of the Association for Computational Linguistics @year: 2012",
        "abstract": "We describe an unsupervised approach to the problem of automatically detecting subgroups of people holding similar opinions in a discussion thread. An intuitive way of identifying this is to detect the attitudes of discussants towards each other or named entities or topics mentioned in the discussion. Sentiment tags play an important role in this detection, but we also note another dimension to the detection of people\u2019s attitudes in a discussion: if two persons share the same opinion, they tend to use similar language content. We consider the latter to be an implicit attitude. In this paper, we investigate the impact of implicit and explicit attitude in two genres of social media discussion data, more formal wikipedia discussions and a debate discussion forum that is much more informal. Experimental results strongly suggest that implicit attitude is an important complement for explicit attitudes (expressed via sentiment) and it can improve the sub-group detection performance independent of genre."
    },
    "P84-1046": {
        "title": "Referring as Requesting @authors: Philip R. Cohen @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 2 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 10 @numCiting: 18 @s2FieldsOfStudy: Computer Science, Philosophy @topics: Amplifier, Relevance, Regular expression @venue: Annual Meeting of the Association for Computational Linguistics @year: 1984",
        "abstract": "Searle 1141 has argued forcefully that referring is a speech act; that people refer, not just expressions. This paper considers what kind of speech act referring might be. I propose a generalization of Searle's &quot;propositional&quot; act of referring that treats it as an illocutionary act, a request, and argue that the propositional act of referring is unnecessary. The essence of the argument is as follows: First, I consider Searle's definition of the propositional act of referring (which I term the PAA, for Propositional Act Account). This definition is found inadequate to deal with various utterances in discourse used for the sole purpose of referring. Although the relevance of such utterances to the propositional act has been defined away by Searle, it is clear that any comprehensive account of referring should treat them. I develop an account of their use in terms of a speaker's requesting the act of referent identification, which is to be understood in a perceptual sense. This illocutionary art analysis (IAA) is shown to satisfy Searle's conditions for referring yet captures utterances that the PAA cannot. The converse position is then examined: Can the IAA capture the same uses of referring expressions as the PAA? If one extends the perceptuallybased notion of referent identification to include Searle's concept of identification, then by associating a complex propositional attitude to one use of the definite determiner, a request can be derived. The IAA thus handles the referring use of definite noun phrases with independently motivated rules. Referring becomes a kind of requesting. Hence, the propositional act of referring is unnecessary."
    },
    "P84-1047": {
        "title": "Entity-Oriented Parsing @authors: P. Hayes @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 1 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 24 @numCiting: 13 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Parsing, Entity, Domain of discourse, Natural language processing, Natural language understanding, Control flow, Cognition, Object type (object-oriented programming), State (computer science), Natural language user interface, Machine translation, Top-down and bottom-up design, Formal system, Tract (literature), Small-outline transistor, Lex (software), Field electron emission @venue: Annual Meeting of the Association for Computational Linguistics @year: 1984",
        "abstract": "An entity-oriented approach to restricted-domain parsing is proposed. In this approach, the definitions of the structure and surface representation of domain entities are grouped. together. Like semantic grammar, this allows easy exploitation of limited domain semantics. In addition, it facilitates fragmentary recognition and the use of multiple parsing strategies, and so is particularly useful for robust recognition of extragrammatical input. Several advantages from the point of view of language definition are also noted. Representative samples from an entity-oriented language definition are presented, along with a control structure for an entity-oriented parser, some parsing strategies that use the control structure, and worked examples of parses. A parser incorporating the control structure and the parsing strategies is currently under implementation."
    },
    "W98-0708": {
        "title": "Linking WordNet Verb Classes to Semantic Interpretation @authors: F. Gomez @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 0 @isOpenAccess: False @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 19 @numCiting: 18 @s2FieldsOfStudy: Computer Science, Linguistics, Computer Science @topics: WordNet, Semantic interpretation @venue: WordNet@ACL/COLING @year: 1998",
        "abstract": "An analysis that defines predicates for Wordnet verb classes and links them to semantic interpretation is presented. The selectional restrictions for the the-, matic roles defining the predicates are WordNet ontological categories. Thematic roles are also linked to the syntactic relations that realize them. The paper illustrates the methodology by providing a detailed analysis of some major WordNet verb classes."
    },
    "P12-2012": {
        "title": "Self-Disclosure and Relationship Strength in Twitter Conversations @authors: Jinyeong Bak, Suin Kim, Alice H. Oh @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 2 @isOpenAccess: False @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 49 @numCiting: 22 @s2FieldsOfStudy: Computer Science, Psychology, Computer Science @topics: Personally identifiable information, Text mining @venue: Annual Meeting of the Association for Computational Linguistics @year: 2012",
        "abstract": "In social psychology, it is generally accepted that one discloses more of his/her personal information to someone in a strong relationship. We present a computational framework for automatically analyzing such self-disclosure behavior in Twitter conversations. Our framework uses text mining techniques to discover topics, emotions, sentiments, lexical patterns, as well as personally identifiable information (PII) and personally embarrassing information (PEI). Our preliminary results illustrate that in relationships with high relationship strength, Twitter users show significantly more frequent behaviors of self-disclosure."
    },
    "W98-0706": {
        "title": "Text Classification Using WordNet Hypernyms @authors: Sam Scott, S. Matwin @citationVelocity: 5 @fieldsOfStudy: Computer Science @influentialCitationCount: 31 @isOpenAccess: False @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 268 @numCiting: 22 @s2FieldsOfStudy: Computer Science, Computer Science @topics: WordNet, Document classification, Statistical classification, Diff utility, Binary classification, Experiment, Rule (guideline), Machine learning, Bag-of-words model, Ripper, Text-based (computing) @venue: WordNet@ACL/COLING @year: 1998",
        "abstract": "This paper describes experiments in Machine Learning for text classification using a new representation of text based on WordNet hypernyms. Six binary classification tasks of varying difficulty are defined, and the Ripper system is used to produce discrimination rules for each task using the new hypernym density representation. Rules are also produced with the commonly used bag-of-words representation, incorporating no knowledge from WordNet. Experiments show that for some of the more difficult tasks the hypernym density representation leads to significantly more accurate and more comprehensible rules."
    },
    "W98-0707": {
        "title": "Towards a Representation of Idioms in WordNet @authors: C. Fellbaum @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 11 @isOpenAccess: False @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 50 @numCiting: 12 @s2FieldsOfStudy: Computer Science, Linguistics @topics: WordNet, Information retrieval, Dictionary, Word sense, Lexicon, Natural language processing, Word-sense disambiguation @venue: WordNet@ACL/COLING @year: 1998",
        "abstract": "WordNet (Miller, 1995), (Fellbaum, 1998) is perhaps the most widely used electronic dictionary of English and serves as the lexicon for a varity of different NLP applications including Information Retrieval (IR), Word Sense Disambiguation (WSD), and Marbine Translation (MT). Despite WordNet's large coverage, which comprises some 100,000 concepts lexicalized by approximately 120,000 word forms (strings) and is comparable to that of a collegiate dictionary, it contains relatively little figurative language. WordNet includes a number of multi-word strings, such as phrasal verbs, but many idiomatic verb phrases like smell a rat, know the ropes, and eat humble pie, are missing. Idioms and metaphors abound in everyday language and are found in texts spanning many genres (see, e.g., (Jadcendoff, 1997) for a numerical estimate of the frequency of idioms and fixed expression). Clearly, a dictionary that includes extended senses of words and phrases is likely to yield more successful NLP applications. On the one hand, no system wants to retrieve the string bucket from the idiom kick the bucket. On the other hand, MT and WSD efforts need to distinguish the sense of ropes in phrases like know/learn/teach someone the ropes from the sense meaning &quot;strong cords&quot;; selecting the latter sense in any of the idiomatic phrases leads to failure. An IR query is likely to be interested only in the &quot;strong cord&quot; reading. When this sense is to be retrieved with the aid of a lexicon intended for multiple applications, the figurative sense must be successfully recognized and excluded from a text that may contain instances of the string ropes with both meanings. In this paper, we consider the possibility of extending WordNet to accommodate figurative meanings in the English lexicon. While much has been written on figurative language, there is no agreement on the boundary between literal and non-literal language, see e.g. (Moon, 1986). Criteria that are commonly accepted include semantic non-compositionality and syntactic constraints on internal modification (such as adjective and adverb insertion) and movement transformations. Our purpose here is not to attempt a clear delimitation or definition of non-literal language, but to examine how extended senses of words and phrases from different syntactic and lexical categories-or conforming to none of the standard categories-are compatible with the network structure of a relational lexicon like WordNet and its particular way of representing words and concepts. Our discussion will focus on, but not be limited to, idiomatic verb phrases."
    },
    "P84-1048": {
        "title": "Combining Functionality and Object-Orientedness\nfor Natural Language Processing @authors: T. Nishida, S. Doshita @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 0 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 3 @numCiting: 6 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Natural language processing, PAL @venue: Annual Meeting of the Association for Computational Linguistics @year: 1984",
        "abstract": "This paper proposes a method for organizing linguistic knowledge in both systematic and flexible fashion. We introduce a purely applicative language (PAL) as an intermediate representation and an object-oriented computation mechanism for its interpretation. PAL enables the establishment of a principled and well-constrained method of interaction among lexicon-oriented linguistic modules. The object-oriented computation mechanism provides a flexible means of abstracting modules and sharing common knowledge."
    },
    "P84-1049": {
        "title": "USE OF HEURISTIC KNOWLEDGE IN CHINESE LANGUAGE ANALYSIS @authors: Yu-lin Lu, Jiantong Zhang @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 0 @isOpenAccess: False @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 7 @numCiting: 68 @s2FieldsOfStudy: Computer Science, Business @topics:  @venue: Engineering Construction and Architectural Management @year: 2021",
        "abstract": "This paper describes an analysis method which uses heuristic knowledge to find local syntactic structures of Chinese sentences. We call it a preprocessing, because we use it before we do global syntactic structure analysiscilof the input sentence. Our purpose is to guide the global analysis through the search space, to avoid unnecessary computation. To realize this, we use a set of special words that appear in commonly used patterns in Chinese. We call them &quot;characteristic words&quot; . They enable us to pick out fragments that might figure in the syntactic structure of the sentence. Knowledge concerning the use of characteristic words enables us to rate alternative fragments, according to pattern statistics, fragment length, distance between characteristic words, and so on. The preprocessing system proposes to the global analysis level a most &quot;likely&quot; partial structure. In case this choice is rejected, backtracking looks for a second choice, and so on. For our system, we use 200 characteristic words. Their rules are written by 101 automata. We tested them against 120 sentences taken from a Chinese physics text book. For this limited set, correct partial structures were proposed as first choice for 94% of sentences. Allowing a 2nd choice, the score is 98%, with a 3rd choice, the score is 100%."
    },
    "W98-0702": {
        "title": "Disambiguating Verbs\nwith the WordNet Category of the Direct Object @authors: Eric V. Siegel @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 0 @isOpenAccess: False @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 12 @numCiting: 26 @s2FieldsOfStudy: Computer Science, Linguistics @topics: WordNet, Text corpus, Numerical analysis @venue: WordNet@ACL/COLING @year: 1998",
        "abstract": "In this paper, I demonstrate that verbs can be disambiguated according to aspect by rules that examine the WordNet category of the direct object. First, when evaluated over a corpus of medical reports, I show that WordNet categories correlate with aspectual class. Then, I develop a rule for distinguishing between stative and event occurrences of have by the WordNet category of the direct object. This rule, which is motivated by both linguistic and statistical analysis, is evaluated over an unrestricted set of nouns. I also show that WordNet categories improve a system that performs aspectual classification with linguistically-based numerical indicators."
    },
    "P12-2011": {
        "title": "Pattern Learning for Relation Extraction with a Hierarchical Topic Model @authors: Enrique Alfonseca, Katja Filippova, Jean-Yves Delort, Guillermo Garrido @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 7 @isOpenAccess: False @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 78 @numCiting: 30 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Topic model, Relationship extraction, Knowledge base, Freebase, Heuristic @venue: Annual Meeting of the Association for Computational Linguistics @year: 2012",
        "abstract": "We describe the use of a hierarchical topic model for automatically identifying syntactic and lexical patterns that explicitly state ontological relations. We leverage distant supervision using relations from the knowledge base FreeBase, but do not require any manual heuristic nor manual seed list selections. Results show that the learned patterns can be used to extract new relations with good precision."
    },
    "W07-1413": {
        "title": "Combining Lexical-Syntactic Information with Machine Learning for\nRecognizing Textual Entailment @authors: Arturo Montejo Raez, Jose Manuel Perea Ortega, Fernando Martinez Santiago, Miguel Angel Garcia Cumbreras, M. T. Martin-Valdivia, L. A. U. Lopez @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 0 @isOpenAccess: False @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 4 @numCiting: 9 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Textual entailment, Machine learning, Binary classification, Experiment, Feature extraction, Statistical classification, Approximation algorithm @venue: ACL-PASCAL@ACL @year: 2007",
        "abstract": "This document contains the description of the experiments carried out by SINAI group. We have developed an approach based on several lexical and syntactic measures integrated by means of different machine learning models. More precisely, we have evaluated three features based on lexical similarity and 11 features based on syntactic tree comparison. In spite of the relatively straightforward approach we have obtained more than 60% for accuracy. Since this is our first participation we think we have reached a good result."
    },
    "A92-1042": {
        "title": "Acquiring and Exploiting the User's Knowledge in Guidance\nInteractions @authors: Eyal Shifroni, U. Ornan @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 0 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 0 @numCiting: 2 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Interaction, dialog, Guidance system @venue: Applied Natural Language Processing Conference @year: 1992",
        "abstract": "This paper presents a model for Flexible Interactive Guidance System (FIGS) that provides people with instructions about natural tasks. The model is developed on the basis of a phenomenological analysis of human guidance and illustrated by a system that gives directions in geographical domains. The instructions are provided through a dialog adapted both in form and content to user's needs. The main problem addressed is how to provide a user\u2014adapted guidance during the normal course of the guidance dialog, without introducing a special time consuming sub-dialog to gain information about the user's state of knowledge. A user-adapted guidance system must collect information about the user's knowledge of the guidance domain and build a User Model (UM). It is known that a UM can improve the behavior of dialog systems and contribute to the ease of their usage and the naturalness of their response ( (Rich, 1979)). However, a UM may also have negative effects on the system's behavior, since the process of its acquisition may increase both the time and the effort the user must invest in the interaction. The model suggested here addresses this problem by weighing the effort required to acquire the UM against the benefits of its usage."
    },
    "A92-1043": {
        "title": "Learning a Scanning Understanding for\n&quot;Real-world&quot; Library Categorization @authors: S. Wermter @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 0 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 0 @numCiting: 6 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Connectionism, Plausibility structure, Chart parser, Lexicon, Categorization, Computer science, Recurrent neural network, Natural language, Context-free grammar, Library classification, Randomness extractor, Microcomputer, Text corpus, Synapomorphy, Word-sense disambiguation, Context-free language, Experiment, Embedded system, Digital history, Extractor (mathematics), Mg (editor) @venue: Applied Natural Language Processing Conference @year: 1992",
        "abstract": "This paper describes, compares, and evaluates three different approaches for learning a semantic classification of library titles: 1) syntactically condensed titles, 2) complete titles, and 3) titles without insignificant words are used for learning the classification in connectionist recurrent plausibility networks. In particular, we demonstrate in this paper that automatically derived feature representations and recurrent plausibility networks can scale up to several thousand library titles and reach almost perfect classification accuracy (>98%) compared to a real-world library classification."
    },
    "A92-1040": {
        "title": "Dialogue Management for Telephone Information Systems @authors: S. McGlashan, N. Fraser, N. Gilbert, E. Bilange, Paul Heisterkamp, N. J. Youd @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 2 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 44 @numCiting: 33 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Dialog system, Information system, Information Systems, Real-time transcription @venue: Applied Natural Language Processing Conference @year: 1992",
        "abstract": "A distributed approach to spoken dialogue management for real-time telephone information systems is outlined."
    },
    "A92-1041": {
        "title": "Lexicon Design Using a Paradigmatic Approach @authors: C. Dumitrescu @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 0 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 1 @numCiting: 6 @s2FieldsOfStudy: Computer Science, Linguistics @topics: Lexicon @venue: Applied Natural Language Processing Conference @year: 1992",
        "abstract": "The paper describes models for representation and methods to handle lexicographic structures supplied by the MORPHO-2 system. It was built to manage monolingual lexicons and to incorporate lexical processing."
    },
    "A92-1046": {
        "title": "The Role of Testing in Grammar Engineering @authors: M. Volk @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 0 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 4 @numCiting: 4 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Software development process, Coherence (physics) @venue: Applied Natural Language Processing Conference @year: 1992",
        "abstract": "In the past grammars have been developed either with an art approach (building up a coherent system; prescriptive grammars like the Latin grammars of the Middle Ages) or with a science approach (describing the laws of nature; descriptive and contrastive grammars). We propose to regard grammar development in Computational Linguistics as an engineering task analogous to software engineerifig: one that requires analysis, specification, implementation, testing, integration and maintenance. The different phases in the software development process correspond to phases in grammar development in the following way: Our project focusses on the testing aspect of the process. Testing can never be exhaustive but must be representative. We therefore propose an archive of test sentences (hence: ATS) to govern the incremental development and the testing of grammar implementations. Towards the goal of a comprehensive collection of test sentences we have restricted ourselves to the construction of an ATS that represents specific syntactic phenomena. The archive aims to be a representative sample of all syntactic phenomena of a natural language, in our case German. The ATS must contain grammatical sentences as well as ungrammatical strings. The grammatical sentences are systematically collected by varying one syntactic phenomenon at a time. E.g. we vary first the subcategory of the verb and then we vary verb tense, etc. For every phenomenon we have to construct ungrammatical strings to check against overgeneration by the grammar. These strings are found by manipulating the plyliomenon in question in such ways as to make it ungrammatical. A syntactic feature must be varied over all values of its domain. E.g. the feature case in German has the values nominative, genitive, dative, and accusative. A noun phrase (NP) that needs to appear in the nominative in a given sentence will then result in three ungrammatical strings when the other cases are assigned. It must be noted that the set of ungrammatical strings gets very large when it comes to problems such as word order where the permutation of all the words in a sentence is necessary to enumerate all possibilities. In this case we need heuristics to find the most appropriate test sentences. E.g. in German the order of NPs in a sentence is variable with certain restrictions whereas the word order within the NP is relatively fixed. Therefore we are much more likely to encounter problems in NP order when setting up our grammar. As a result we will have to focus on ungrammatical strings with NP order problems. In contrast to grammatical sentences ungrammatical strings are only seldom found naturally (e.g. errors of language learners) and it will be interesting to study whether these occurrences (at least the most frequent ones) correspond to our constructed examples. The judgement of grammatical versus ungrammatical strings is subjective and has little to say about the acceptability of a sentence in a real-world communication. Thus our ATS will model competence rather than performance in the Chomskyan sense. For the practical use the ATS must be organized in a modular fashion, enabling the user to adapt and extend the archive according to his needs and convictions. Furthermore it must be documented why a sentence has been entered into the archive, since every sentence displays a multitude of syntactic information, only some of which is relevant in our domain."
    },
    "A92-1047": {
        "title": "Lexical Processing in the CLARE System @authors: D. Carter @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 0 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 0 @numCiting: 0 @s2FieldsOfStudy: Computer Science, Linguistics @topics: Parsing, Speech recognition, Telephone number, Nondeterministic algorithm, Email, Spaces, Japanese input methods @venue: Applied Natural Language Processing Conference @year: 1992",
        "abstract": "In many language processing systems, uncertainty in the boundaries of linguistic units means that data are represented not as a well-defined sequence of units but as a lattice of possibilities. This is often the case in speech recognition, syntactic parsing and Japanese kana-kanji conversion. In contrast, however, it is often assumed that, for languages written with interword spaces, it is sufficient to prepare an input character stream for parsing by grouping it deterministically into a sequence of words, punctuation symbols and perhaps other items. But for typed input, spaces do not necessarily correspond to boundaries between lexical items, because of errors and other, linguistic, phenomena. This means that a lattice representation, not a simple sequence, should be used throughout front end (pre-parsing) analysis. The CLARE system under development at SRI Cambridge uses such a representation, allowing it to deal straightforwardly with combinations or multiple occurrences of phenomena that would be difficult or impossible to process correctly under a sequence representation. This paper concentrates on CLARE's ability to deal with typing and spelling errors, which are especially common in interactive use, for which CLARE is designed. The word identity and word boundary ambiguities encountered in the interpretation of errorful input often require the application of syntactic and semantic knowledge on a phrasal or even sentential scale. Such knowledge may be applied as soon as the problem is encountered; however, this brings major problems with it, such as the need for adequate lookahead, and the difficulties of engineering large systems where the processing levels are tightly coupled. To avoid such problems, CLARE adopts a staged architecture, in which indeterminacy is preserved until the knowledge needed to resolve it is ready to be applied. An appropriate representation is of course the key to doing this efficiently."
    },
    "A92-1044": {
        "title": "SEISD: An environment for extraction of\nSemantic Information from on-line dictionaries @authors: A. Ageno, Irene Castellon, M. A. Marti, German Rigau, Francesc Ribas Framis, H. Rodriguez, M. Taule, M. Verdejo @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 1 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 17 @numCiting: 6 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Lexical database, Knowledge-based systems, Vagueness, Heuristic (computer science), Knowledge acquisition, Natural language processing, Machine-readable dictionary, Online and offline, Programming paradigm, Human-readable medium @venue: Applied Natural Language Processing Conference @year: 1992",
        "abstract": "Knowledge Acquisition constitutes a main problem as regards the development of real Knowledge-based systems. This problem has been dealt with in a variety of ways. One of the most promising paradigms is based on the use of already existing sources in order to extract knowledge from them semiautomatically which will then be used in Knowledge-based applications. The Acquilex Project, within which we are working, follows this paradigm. The basic aim of Acquilex is the development of techniques and methods in order to use Machine Readable Dictionaries (MRD) * for building lexical components for Natural Language Processing Systems. SEISD (Sistema de Extraccion de Informacion Semantica de Diccionarios) is an environment for extracting semantic information from MRDs [Ageno et al. 91b]. The system takes as its input a Lexical Database (LDB) where all the information contained in the MRD has been stored in an structured format. The extraction process is not fully automatic. To some extent, the choices made by the system must be both validated and confirmed by a human expert. Thus, an interactive environment must be used for performing such a task. One of the main contribution of our system lies in the way it guides the interactive process, focusing on the choice points and providing access to the information relevant to decision taking. System performance is controlled by a set of weighted heuristics that supplies the lack of algorithmic criteria or their vagueness in several crucial decision points. We will now summarize the most important characteristics of our system:"
    },
    "A92-1045": {
        "title": "Multi-Purpose Development and Operation\nEnvironments\nfor Natural Language Applications @authors: S. Nirenburg, Peter Shell, A. Cohen, Peter Cousseau, D. Grannes, Chris McNeilly @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 0 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 14 @numCiting: 11 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Workstation, Natural language processing, User interface, Motif, Dictionary, Modem, IBM Tivoli Workload Scheduler, Knowledge acquisition, Machine translation, X Window System, Ontology learning, Computer, Circuit complexity, Microsoft Windows, Interactivity, Window function, Channel length modulation, General-purpose modeling @venue: Applied Natural Language Processing Conference @year: 1992",
        "abstract": "Interactive user environments have been a central efficiencyenhancing feature of many modern computer applications, including natural language processing. There are two major classes of users for whom NLP environments can be constructed \u2014 developers and end users, such as technical writers and translators. Developers need help in the various knowledge acquisition tasks, such as dictionary and grammar writing for NLP applications. End users look for efficiency enhancements in document preparation beyond the level of word processing support. There are two approaches to the solution of this problem. A dedicated workstation can be developed for each of the required functionalities. Alternatively, workstations can be configured as sets of application routines attached to a universal user interface. In this report we describe a general-purpose user environment, under development at the Center for Machine Translation of Carnegie Mellon University, capable of supporting a number of dedicated workstation configurations. Among the types of end users whom this system will benefit are technical writers, text revisors and translators. In the framework of NLP system development this tool supports dictionary and ontology acquisition. A number of separate functionalities included in this system have been developed and used either in commercial word processing software packages or in NLP projects (e.g., the translator's tools described in Macklovitch, 1989,; and the developer environments IRACQ (Ayuso et al., 1987), LUKE (Wroblewski and Rich, 1988) or ONTOS (Monarch and Nirenburg, 1989), among many others). Our system allows a merge of the two directions in the tool development. One direct reason to put the two previously separate kinds of functionality into a single system was to support the knowledge-based machineaided translation environment which involves an interactive human editor who uses an interface to help the machine understand the source text. A standard Unix- and X-windows-based workstation platform has been selected for our system, whose working name is TWS, for &quot;Translator's Workstation.&quot; We also used the C-based X11 toolkit called MOTIF (Motif, 1991) and its CommonLisp interface called CLM (Babatz et. al., 1991). In practice, TWS consists of a number of application (functionality) modules which are integrated through the central MOTIF-based user interface module. For reasons of uniformity, each of the applications uses the facilities of the user interface for display and input. Each module uses a standard window to interact with the user, and each window has standard menus which, among other functionalities, allow the user to invoke any other module. Each module also has special menus. The architecture of TWS is presented in Figure 1. We illustrate the basic interface and several application utilities that have been connected to the TWS. Specifically, we illustrate the text editor (an end-user functionality), the utility for statistical processing of corpora (useful both for end users and for developers) and the ontology acquisition and maintenance tool, based on ONTOS (a developer functionality). Due to the lack of space, we do not describe our interface to reference sources, such as online dictionaries or user-defined glossaries, or our interface-building interface which allows both the TWS developer and TWS user define and modify the user interfaces the system uses. TWS is designed in such a way that various utilities can be imported into the system, so that there is no need to implement every functionality locally. One example of an existing imported module in TWS is a set of bilingual text alignment routines. The concept of importing utilities is very important to our work. The user interface facilities of the kind we developed makes it easy to import applications with a minimum of effort and with a capability of making them &quot;look and feel&quot; similarly to the rest of the applications in the system. In our future work we intend to mix the development of utilities with importing and integrating existing applications, whenever that is desirable and feasible. Figure 2 shows a basic editing window of TWS. The central feature of the TWS editor is its capability to emulate various existing word processors. This feature is important from the standpoint of acceptance of the workstation environments by end users. Currently the editor emulates WORD PERFECTIm and EMACS. Figure 3 illustrates the corpus processing functionality. It can be used by end users to compare the usage of a term in various places in a set of documents and by the developer to gain insights for dictionary and ontology acquisition. the left shows frequency counts for each word in the chosen text, while the right sub-window shows the context of selected words. A detailed discussion of the ONTOS system see in Monarch, 1989, Carlson and Nirenbuig, 1991. To include the ONTOS functionality into TWS, a new grapher has been built. The grapher facilitates browsing through, editing, creating and deleting frames. An important feature is its ability to handle graphs rather than trees, so that multiple inheritance can be graphically represented. The ONTOS module inside TWS also supports a structured editor for manipulating ontological concepts symbolically. We believe that the TWS GRAPHER compares favorably with other graphers, notably the AGGREGRAPHS (see Dannenberg et. al., 1991) which is part of the GARNET system and the ISI GRAPHER (see Robins, 1988). Unlike AGGREGRAPHS, which can only display a knowledge base, our grapher lets the user edit it. As for the ISI GRAPHER, the latter cannot easily support more than one type of link between nodes, while the TWS grapher can support an unlimited number of links, as well as validity and consistency tests for them."
    },
    "P12-2017": {
        "title": "Using Rejuvenation to Improve Particle Filtering for Bayesian Word\nSegmentation @authors: Benjamin Borschinger, Mark Johnson @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 0 @isOpenAccess: False @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 15 @numCiting: 10 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Particle filter, Text segmentation, Algorithm @venue: Annual Meeting of the Association for Computational Linguistics @year: 2012",
        "abstract": "We present a novel extension to a recently proposed incremental learning algorithm for the word segmentation problem originally introduced in Goldwater (2006). By adding rejuvenation to a particle filter, we are able to considerably improve its performance, both in terms of finding higher probability and higher accuracy solutions."
    },
    "S14-2119": {
        "title": "tucSage: Grammar Rule Induction for Spoken Dialogue Systems via\nProbabilistic Candidate Selection @authors: Arodami Chorianopoulou, Georgia Athanasopoulou, Elias Iosif, I. Klasinas, A. Potamianos @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 0 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 0 @numCiting: 27 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Grammar induction, Rule induction, Statistical model, Software portability, Selection algorithm, Experiment, Semantic similarity, Dialog system @venue: International Workshop on Semantic Evaluation @year: 2014",
        "abstract": "We describe the grammar induction system for Spoken Dialogue Systems (SDS) submitted to SemEval\u201914: Task 2. A statistical model is trained with a rich feature set and used for the selection of candidate rule fragments. Posterior probabilities produced by the fragment selection model are fused with estimates of phraselevel similarity based on lexical and contextual information. Domain and language portability are among the advantages of the proposed system that was experimentally validated for three thematically different domains in two languages."
    },
    "S14-2118": {
        "title": "TMUNSW: Disorder Concept Recognition and Normalization in\nClinical Notes for SemEval-2014 Task 7\nJitendra\nJonnagaddala @authors: J. Jonnagaddala, Manish Kumar, Hong-Jie Dai, Enny Rachmani, Chien-Yeh Hsu @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 1 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 4 @numCiting: 13 @s2FieldsOfStudy: Computer Science, Computer Science, Psychology @topics: SemEval, Conditional random field, Error analysis (mathematics), Algorithm, Named entity, Pattern matching, Identifier, Baseline (configuration management), F1 score @venue: International Workshop on Semantic Evaluation @year: 2014",
        "abstract": "We present our participation in Task 7 of SemEval shared task 2014. The goal of this particular task includes the identification of disorder named entities and the mapping of each disorder to a unique Unified Medical Language System concept identifier, which were referred to as Task A and Task B respectively. We participated in both of these subtasks and used YTEX as a baseline system. We further developed a supervised linear chain Conditional Random Field model based on sets of features to predict disorder mentions. To take benefit of results from both systems we merged these results. Under strict condition our best run evaluated at 0.549 F-measure for Task A and an accuracy of 0.489 for Task B on test dataset. Based on our error analysis we conclude that recall of our system can be significantly increased by adding more features to the Conditional Random Field model and by using another type of tag representation or frame matching algorithm to deal with the disjoint entity mentions."
    },
    "W11-1300": {
        "title": "Distributional Semantics and Compositionality (DiSCo\u20192011)\nWorkshop at ACL HLT 2011\nProceedings of the Workshop",
        "abstract": "Any NLP system that does semantic processing relies on the assumption of semantic compositionality: the meaning of a phrase is determined by the meanings of its parts and their combination. For this, it is necessary to have automatic methods that are capable to reproduce the compositionality of language. Recent years have shown the renaissance of interest in distributional semantics. While distributional methods in semantics have proven to be very efficient in tackling a wide range of tasks in natural language processing, e.g., document retrieval, clustering and classification, question answering, query expansion, word similarity, synonym extraction, relation extraction, and many others, they are still strongly limited by being inherently word-based. The main hurdle for vector space models to further progress is the ability to handle compositionality. The workshop is of potential interest to the researchers working on distributional semantics and compositionality as well as for those interested in extracting non-compositional phrases from large corpora by applying distributional methods that assign a graded compositionality score to a phrase. This score denotes the extent to which the compositionality assumption holds for a given expression. The latter can be used, for example, to decide whether the phrase should be treated as a single unit in applications or included in a dictionary. We have emphasized that the focus is on automatically acquiring semantic compositionality, thereby explicitly avoiding approaches that employ prefabricated lists of non-compositional phrases. This volume contains papers accepted for publication at DiSCo\u20192011 Workshop on Distributional Semantics and Compositionality, collocated with ACL-HLT 2011, the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. The workshop consists of a main session and a shared task. To the best of our knowledge, this has been the first attempt in the community to offer a dataset and a shared task that allows to explicitly evaluate the models of graded compositionality for phrases per se that occur in three types of grammatical relations: adjective-noun pairs, subject-verb and verb-object pairs in English and German. For the main session, one long and two short papers have been accepted for publication. Further, seven teams with 19 systems have taken part in the shared task. We consider this a success, taking into consideration that the task is new and difficult. The description of the task and the results of evaluation are part of these proceedings. In short, approaches ranging from pure statistical association measures to various variations of word space models have been applied to solve the DiSCo task. Six system description papers have been accepted for publication. Both regular and system description papers have been carefully reviewed by the program committee. We would like to thank the committee for insightful and timely reviews (in spite of the Easter holidays). The accepted regular articles address a rather wide spectrum of issues within distributional semantics, such as: iii four compositional methods for distributional vectors (Vecchi, Baroni and Zamparelli, 2011); Last but not least, we would like to thank Dominic Widdows for agreeing to give an invited talk about the theory and practice behind some of recent developments in semantic vectors. Enjoy the workshop! The organizers:"
    },
    "P12-2016": {
        "title": "Decoding Running Key Ciphers @authors: S. Reddy, Kevin Knight @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 0 @isOpenAccess: False @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 0 @numCiting: 12 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Cipher, Gibbs sampling, Natural language processing, Decipherment, Language model, Text-based (computing), Line code, Encryption, Sampling (signal processing), Viterbi algorithm, Dhrystone, Microsoft Word for Mac @venue: Annual Meeting of the Association for Computational Linguistics @year: 2012",
        "abstract": "There has been recent interest in the problem of decoding letter substitution ciphers using techniques inspired by natural language processing. We consider a different type of classical encoding scheme known as the running key cipher, and propose a search solution using Gibbs sampling with a word language model. We evaluate our method on synthetic ciphertexts of different lengths, and find that it outperforms previous work that employs Viterbi decoding with character-based models."
    },
    "W13-2414": {
        "title": "Recognition of Named Entities Boundaries in Polish Texts @authors: Michal Marcinczuk, Jan Kocon @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 0 @isOpenAccess: False @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 6 @numCiting: 26 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Partial evaluation, Precision and recall, Entity, Video post-processing, North American Mesoscale Model, Database trigger, NAM, Categorization, Heuristic (computer science), Software propagation, Named-entity recognition @venue: BSNLP@ACL @year: 2013",
        "abstract": "In the paper we discuss the problem of low recall for the named entity (NE) recognition task for Polish. We discuss to what extent the recall of NE recognition can be improved by reducing the space of NE categories. We also present several extensions to the binary model which give an improvement of the recall. The extensions include: new features, application of external knowledge and post-processing. For the partial evaluation the final model obtained 90.02% recall with 91.30% precision on the corpus of economic news."
    },
    "P12-2015": {
        "title": "A Context-sensitive, Multi-faceted model of Lexico-Conceptual Affect",
        "abstract": "Since we can \u2018spin\u2019 words and concepts to suit our affective needs, context is a major determinant of the perceived affect of a word or concept. We view this re-profiling as a selective emphasis or de-emphasis of the qualities that underpin our shared stereotype of a concept or a word meaning, and construct our model of the affective lexicon accordingly. We show how a large body of affective stereotypes can be acquired from the web, and also show how these are used to create and interpret affective metaphors."
    },
    "P06-2058": {
        "title": "Obfuscating Document Stylometry to Preserve Author Anonymity @authors: Gary Kacmarcik, Michael Gamon @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 7 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 95 @numCiting: 12 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Stylometry, Data anonymization, Feature selection, Test set @venue: Annual Meeting of the Association for Computational Linguistics @year: 2006",
        "abstract": "This paper explores techniques for reducing the effectiveness of standard authorship attribution techniques so that an author A can preserve anonymity for a particular document D. We discuss feature selection and adjustment and show how this information can be fed back to the author to create a new document D\u2019 for which the calculated attribution moves away from A. Since it can be labor intensive to adjust the document in this fashion, we attempt to quantify the amount of effort required to produce the anonymized document and introduce two levels of anonymization: shallow and deep. In our test set, we show that shallow anonymization can be achieved by making 14 changes per 1000 words to reduce the likelihood of identifying A as the author by an average of more than 83%. For deep anonymization, we adapt the unmasking work of Koppel and Schler to provide feedback that allows the author to choose the level of anonymization."
    },
    "P06-2059": {
        "title": "Automatic Construction of Polarity-tagged Corpus from HTML\nDocuments @authors: Nobuhiro Kaji, M. Kitsuregawa @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 4 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 83 @numCiting: 15 @s2FieldsOfStudy: Computer Science, Computer Science @topics: HTML, Text corpus @venue: Annual Meeting of the Association for Computational Linguistics @year: 2006",
        "abstract": "This paper proposes a novel method of building polarity-tagged corpus from HTML documents. The characteristics of this method is that it is fully automatic and can be applied to arbitrary HTML documents. The idea behind our method is to utilize certain layout structures and linguistic pattern. By using them, we can automatically extract such sentences that express opinion. In our experiment, the method could construct a corpus consisting of 126,610 sentences."
    },
    "P06-2056": {
        "title": "UnsupervisedSegmentationofChineseText byUseofBranchingEntropy",
        "abstract": "We propose an unsupervised segmentation method based on an assumption about language data: that the increasing point of entropy of successive characters is the location of a word boundary. A large-scale experiment was conducted by using 200 MB of unsegmented training data and 1 MB of test data, and precision of 90% was attained with recall being around 80%. Moreover, we found that the precision was stable at around 90% independently of the learning data size. 1Introduction The theme of this paper is the following assumption: The uncertainty of tokens coming after a sequence helps determine whether a given position is at a boundary. (A) Intuitively, as illustrated in Figure 1, the variety of successive tokens at each character inside a word monotonically decreases according to the offset length, because the longer the preceding character n-gram, the longer the preceding context and the more it restricts the appearance of possible next tokens. For example, it is easier to guess which character comes after &quot;natura&quot; than after &quot;na&quot; . On the other hand, the uncertainty at the position of a word border becomes greater, and the complexity increases, as the position is out of context. With the same example, it is difficult to guess which character comes after &quot;natural &quot;. This suggests that a word border can be detected by focusing on the differentials of the uncertaintyofbranching. In this paper, we report our study on applying this assumption to Chinese word segFigure1:Intuitiveillustrationofavarietyof successivetokensandawordboundary mentation by formalizing the uncertainty of successive tokens via the branching entropy (which we mathematically define in the next section). Our intention in this paper is above all to study the fundamental and scientific statistical property underlying language data, so that it can be applied to language engineering. The above assumption (A) dates back to the fundamental work done by Harris (Harris, 1955), where he says that when the number of different tokens coming after every prefix of a word marks the maximum value, then the location corresponds to the morpheme boundary. Recently, with the increasing availability of corpora, this property underlying language has been tested through segmentation into words and morphemes. Kempe (Kempe, 1999) reports a preliminary experiment to detect word borders in German and English texts by monitoring the entropy of successive characters for 4-grams. Also, the second author of this paper (Tanaka-Ishii, 2005) have shown how Japanese and Chinese can be segmented into words by formalizing the uncertainty with the branching entropy. Even though the test data was limited to a small amount in this work, the report suggested how assumption (A) holds better when each of the sequence elements forms a semantic unit. This motivated our work to conduct a further, larger-scale test in the Chinese language, which is the only human language consisting entirely of ideograms (i.e., semantic units). In this sense, the choice of Chinese as the language in our work is essential. If the assumption holds well, the most important and direct application is unsupervised text segmentation into words. Many works in unsupervised segmentation so far could be interpreted as formulating assumption (A) in a similar sense where branching stays low inside words but increases at a word or morpheme border. None of these works, however, is directly based on (A), and they introduce other factors within their overall methodologies. Some works are based on in-word branching frequencies formulated in an original evaluation function, as in (Ando and Lee, 2000) (boundary precision =84.5%, recall= 78.0%, tested on 12500 Japanese ideogram words). Sun et al. (Sun et al., 1998) uses mutual information (boundary p=91.8%, no report for recall, 1588 Chinese characters), and Feng(Feng et al., 2004) incorporates branching counts in the evaluation function to be optimized for obtaining boundaries (word precision=76%, recall=78%, 2000 sentences). From the performance results listed here, we can see that unsupervised segmentation is more difficult, by far, than supervised segmentation; therefore, the algorithms are complex, and previous studies have tended to be limited in terms of both the test corpus size and the target. In contrast, as assumption (A) is simple, we keep this simplicity in our formalization and directly test the assumption on a large-scale test corpus consisting of 1001 KB manually segmented data with the training corpus consisting of 200 MB of Chinese text. Chinese is such an important language that supervised segmentation methods are already very mature. The current state-of-the-art segmentation software developed by (Low et al., 2005), which ranks as the best in the SIGHAN bakeoff (Emerson, 2005), attains word precision and recall of 96.9% and 96.8%, respectively, on the PKU track. There is also free Figure2:DecreaseinH(XjXn)forChinese characterswhennisincreased softwaresuchas(Zhangetal.,2003)whose performance is also high. Even then, as most supervised methods learn on manually segmented newspaper data, when the input text is not from newspapers, the performance can be insufficient. Given that the construction of learning data is costly, we believe the performance can be raised by combining the supervised and unsupervised methods. Consequently, this paper verifies assumption (A) in a fundamental manner for Chinese text and addresses the questions of why and to what extent (A) holds, when applying it to the Chinese word segmentation problem. We first formalize assumption (A) in a general manner."
    },
    "P06-2057": {
        "title": "A FrameNet-based Semantic Role Labeler for Swedish @authors: Richard Johansson, P. Nugues @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 2 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 50 @numCiting: 16 @s2FieldsOfStudy: Computer Science, Computer Science @topics: FrameNet, Parallel text, Semantic role labeling, Label printer applicator, Parsing, Precision and recall, HTML element, Test set, Text corpus, Algorithm @venue: Annual Meeting of the Association for Computational Linguistics @year: 2006",
        "abstract": "We present a FrameNet-based semantic role labeling system for Swedish text. As training data for the system, we used an annotated corpus that we produced by transferring FrameNet annotation from the English side to the Swedish side in a parallel corpus. In addition, we describe two frame element bracketing algorithms that are suitable when no robust constituent parsers are available. We evaluated the system on a part of the FrameNet example corpus that we translated manually, and obtained an accuracy score of 0.75 on the classification of presegmented frame elements, and precision and recall scores of 0.67 and 0.47 for the complete task."
    },
    "P06-2054": {
        "title": "Exploiting Non-local Features for Spoken Language Understanding @authors: Minwoo Jeong, G. G. Lee @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 2 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 20 @numCiting: 14 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Algorithm, Mutual information, Natural language understanding, Information extraction, Named-entity recognition, Supervised learning, Parsing, Named entity, Email, Conditional random field, ENCODE @venue: Annual Meeting of the Association for Computational Linguistics @year: 2006",
        "abstract": "In this paper, we exploit non-local features as an estimate of long-distance dependencies to improve performance on the statistical spoken language understanding (SLU) problem. The statistical natural language parsers trained on text perform unreliably to encode non-local information on spoken language. An alternative method we propose is to use trigger pairs that are automatically extracted by a feature induction algorithm. We describe a light version of the inducer in which a simple modification is efficient and successful. We evaluate our method on an SLU task and show an error reduction of up to 27% over the base local model."
    },
    "P06-2055": {
        "title": "Analysis and Repair of Name Tagger Errors @authors: Heng Ji, R. Grishman @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 1 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 21 @numCiting: 9 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Brill tagger, Natural language processing, Relationship extraction, Information extraction, Ontology components, Parsing, Pipeline (computing) @venue: Annual Meeting of the Association for Computational Linguistics @year: 2006",
        "abstract": "Name tagging is a critical early stage in many natural language processing pipelines. In this paper we analyze the types of errors produced by a tagger, distinguishing name classification and various types of name identification errors. We present a joint inference model to improve Chinese name tagging by incorporating feedback from subsequent stages in an information extraction pipeline: name structure parsing, cross-document coreference, semantic relation extraction and event extraction. We show through examples and performance measurement how different stages can correct different types of errors. The resulting accuracy approaches that of individual human annotators."
    },
    "P06-2052": {
        "title": "Efficient sentence retrieval based on syntactic structure @authors: Hiroshi Ichikawa, Keita Hakoda, Taiichi Hashimoto, T. Tokunaga @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 1 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 6 @numCiting: 10 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Structural similarity, Computation, Experiment, Approximation algorithm @venue: Annual Meeting of the Association for Computational Linguistics @year: 2006",
        "abstract": "This paper proposes an efficient method of sentence retrieval based on syntactic structure. Collins proposed Tree Kernel to calculate structural similarity. However, structual retrieval based on Tree Kernel is not practicable because the size of the index table by Tree Kernel becomes impractical. We propose more efficient algorithms approximating Tree Kernel: Tree Overlapping and Subpath Set. These algorithms are more efficient than Tree Kernel because indexing is possible with practical computation resources. The results of the experiments comparing these three algorithms showed that structural retrieval with Tree Overlapping and Subpath Set were faster than that with Tree Kernel by 100 times and 1,000 times respectively."
    },
    "P06-2053": {
        "title": "Towards the Orwellian Nightmare\nSeparation of Business and Personal Emails @authors: Sanaz Jabbari, B. Allison, David Guthrie, Louise Guthrie @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 3 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 21 @numCiting: 7 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Email, Inter-rater reliability, Maximal set, Humans, Eventual consistency @venue: Annual Meeting of the Association for Computational Linguistics @year: 2006",
        "abstract": "This paper describes the largest scale annotation project involving the Enron email corpus to date. Over 12,500 emails were classified, by humans, into the categories \u201cBusiness\u201d and \u201cPersonal\u201d, and then subcategorised by type within these categories. The paper quantifies how well humans perform on this task (evaluated by inter-annotator agreement). It presents the problems experienced with the separation of these language types. As a final section, the paper presents preliminary results using a machine to perform this classification task."
    },
    "P06-2050": {
        "title": "When Conset meets Synset: A Preliminary Survey of an Ontological\nLexical Resource based on Chinese Characters @authors: S. Hsieh, Chu-Ren Huang @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 0 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 5 @numCiting: 14 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Synonym ring, Computational linguistics, Theory, Natural language processing, Goto, Chinese room @venue: Annual Meeting of the Association for Computational Linguistics @year: 2006",
        "abstract": "This paper describes an on-going project concerning with an ontological lexical resource based on the abundant conceptual information grounded on Chinese characters. The ultimate goal of this project is set to construct a cognitively sound and computationally effective character-grounded machine-understandable resource. Philosophically, Chinese ideogram has its ontological status, but its applicability to the NLP task has not been expressed explicitly in terms of language resource. We thus propose the first attempt to locate Chinese characters within the context of ontology. Having the primary success in applying it to some NLP tasks, we believe that the construction of this knowledge resource will shed new light on theoretical setting as well as the construction of Chinese lexical semantic resources."
    },
    "P06-2051": {
        "title": "Spontaneous Speech Understanding for Robust Multi-Modal\nHuman-Robot Communication @authors: S. Huwel, B. Wrede @citationVelocity: 0 @fieldsOfStudy: Computer Science @influentialCitationCount: 1 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 18 @numCiting: 24 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Spontaneous order, Situated, Dialog manager, Modal logic, Semantic interpretation @venue: Annual Meeting of the Association for Computational Linguistics @year: 2006",
        "abstract": "This paper presents a speech understanding component for enabling robust situated human-robot communication. The aim is to gain semantic interpretations of utterances that serve as a basis for multi-modal dialog management also in cases where the recognized word-stream is not grammatically correct. For the understanding process, we designed semantic processable units, which are adapted to the domain of situated communication. Our framework supports the specific characteristics of spontaneous speech used in combination with gestures in a real world scenario. It also provides information about the dialog acts. Finally, we present a processing mechanism using these concept structures to generate the most likely semantic interpretation of the utterances and to evaluate the interpretation with respect to semantic coherence."
    },
    "P15-1053": {
        "title": "Modeling Argument Strength in Student Essays @authors: Isaac Persing, Vincent Ng @citationVelocity: 13 @fieldsOfStudy: Computer Science @influentialCitationCount: 6 @isOpenAccess: True @isPublisherLicensed: True @is_publisher_licensed: True @numCitedBy: 126 @numCiting: 21 @s2FieldsOfStudy: Computer Science, Computer Science @topics: Baseline (configuration management), Software feature, Heuristic, Supervised learning @venue: Annual Meeting of the Association for Computational Linguistics @year: 2015",
        "abstract": "While recent years have seen a surge of interest in automated essay grading, including work on grading essays with respect to particular dimensions such as prompt adherence, coherence, and technical quality, there has been relatively little work on grading the essay dimension of argument strength, which is arguably the most important aspect of argumentative essays. We introduce a new corpus of argumentative student essays annotated with argument strength scores and propose a supervised, feature-rich approach to automatically scoring the essays along this dimension. Our approach significantly outperforms a baseline that relies solely on heuristically applied sentence argument function labels by up to 16.1%."
